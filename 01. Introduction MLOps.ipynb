{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cab3c5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## MLOps Cycle and Vertex AI Features\n",
    "\n",
    "The MLOps cycle generally consists of stages like Data Preparation, Model Development (Training & Evaluation), Model Deployment, and Model Monitoring & Governance. Vertex AI provides specialized tools for each.\n",
    "\n",
    "Vertex AI is designed to support the entire MLOps lifecycle, providing a unified platform for building, deploying, and managing machine learning models. \n",
    "\n",
    "**Notice:** if you find this emoji üßë‚Äçüéì it means that you will see more on this in this training\n",
    "\n",
    "Here's how its key features align with the MLOps cycle as of mid-2025:\n",
    "\n",
    "\n",
    "### 1. Data Preparation and Management\n",
    "\n",
    "**Goal:** To collect, clean, transform, and store data for machine learning. This includes managing features for reusability and consistency.\n",
    "\n",
    "* **üìÅVertex AI Feature Store:**\n",
    "    * **Purpose:** A centralized repository for organizing, storing, and serving machine learning features. It helps in reusing features across multiple models and teams, ensuring consistency between training and serving, and reducing feature engineering efforts.\n",
    "    * **Key Capabilities (as of 2025):**\n",
    "        * **Online Serving:** Provides ultra-low latency access to the latest feature values for real-time predictions. This is crucial for applications like fraud detection or personalized recommendations. Optimized online serving is recommended for most scenarios, offering lower latencies than Bigtable online serving, and supports embeddings management.\n",
    "        * **Offline Serving:** Enables batch retrieval of historical feature data for model training and batch predictions, ensuring point-in-time correctness to prevent data leakage.\n",
    "        * **Integration with BigQuery:** Features are managed from BigQuery tables or views, acting as the offline store. Vertex AI Feature Store acts as a metadata layer interfacing with these BigQuery data sources.\n",
    "        * **Feature Monitoring:** Capabilities to monitor feature values for drift and anomalies, which can indicate issues with data pipelines or changes in data distribution that might impact model performance.\n",
    "        * **Embedding Management:** Supports storing and serving embeddings (vector representations of data) directly, crucial for vector similarity search and retrieval-augmented generation (RAG) applications.\n",
    "        * **Feature Registry:** For setting up and managing metadata about your features, including feature groups and feature views.\n",
    "\n",
    "* **üìÅCloud Storage:**\n",
    "    * **Purpose:** Highly scalable and durable object storage.\n",
    "    * **Role in MLOps:** Used for storing various ML artifacts, including raw datasets, preprocessed data, model artifacts (trained models), and pipeline outputs.\n",
    "\n",
    "  \n",
    "* **üìÅ - üßë‚Äçüéì Vertex AI Datastore (specifically for Generative AI Applications/Vertex AI Search):**\n",
    "    * **Purpose:** While not a general-purpose ML data store like BigQuery, Vertex AI Datastore is a crucial component within **Vertex AI Search** and **Vertex AI Conversation** (now **Vertex AI Agent Garden**). It's designed to ingest and manage various types of data (website, structured, unstructured, healthcare FHIR) to be used for grounding generative AI models, enabling accurate and context-aware responses.\n",
    "    * **Key Capabilities:**\n",
    "        * **Website Data:** Indexes data from public or private websites for search.\n",
    "        * **Structured Data:** For semantic search or recommendations over structured data, imported from BigQuery or Cloud Storage.\n",
    "        * **Unstructured Data:** Supports ingesting and searching over various document types (PDFs, text files).\n",
    "        * **Healthcare FHIR Data:** Specialized data store for healthcare data.\n",
    "        * **Blended Search:** Allows combining multiple data sources for comprehensive search results.\n",
    "        * **Grounding:** Provides the factual basis for generative AI models, reducing hallucinations by linking model responses to specific data.\n",
    "    * **Example:** https://console.cloud.google.com/vertex-ai/locations/europe-west1/datasets/2393654405854396416/analyze?inv=1&invt=Ab38cA&project=poc-eurobet-ds\n",
    "\n",
    "* **üìÅ‚öôÔ∏è - üßë‚Äçüéì  BigQuery:**\n",
    "    * **Purpose:** Google Cloud's fully managed, petabyte-scale data warehouse. It serves as the foundational data storage for most ML workflows on Vertex AI.\n",
    "    * **Role in MLOps:** Used for storing raw data, transformed data, and features that feed into the Feature Store. Its scalability and analytical capabilities make it ideal for large-scale data preparation.\n",
    "  \n",
    "* **‚öôÔ∏è - üßë‚Äçüéì  Dataproc:**\n",
    "    * **Purpose:** A managed service for running Apache Spark, Hadoop, Flink, and Presto clusters.\n",
    "    * **Role in MLOps:** For large-scale data processing, transformations, and feature engineering tasks, especially when complex batch processing is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702e699",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Model Development (Training & Evaluation)\n",
    "\n",
    "**Goal:** To develop, train, and evaluate machine learning models, iterating on different architectures and hyperparameters.\n",
    "\n",
    "* **üßë‚Äçüéì - Vertex AI Workbench:**\n",
    "    * **Purpose:** A fully managed, unified development environment for data scientists and ML engineers. It provides Jupyter-managed notebooks.\n",
    "    * **Role in MLOps:** Serves as the primary interface for interactive development, experimentation, data exploration, and model prototyping. It integrates seamlessly with other Vertex AI services.\n",
    "* **üßë‚Äçüéì - Vertex AI Training:**\n",
    "    * **Purpose:** Offers managed training jobs for custom models and AutoML capabilities.\n",
    "    * **Key Capabilities:**\n",
    "        * **Custom Training:** Allows users to run training jobs using custom code (TensorFlow, PyTorch, scikit-learn, XGBoost, etc.) on scalable infrastructure, including GPUs and TPUs (Ironwood TPUs for optimized performance). Supports distributed training.\n",
    "        * **AutoML:** For users with limited ML expertise, AutoML automates model selection, hyperparameter tuning, and architecture search for various data types (tabular, image, text, video).\n",
    "        * **Hyperparameter Tuning (Vertex AI Vizier):** A black-box optimization service that automates the process of finding the best hyperparameters for a model, significantly accelerating experimentation.\n",
    "        * **Generative AI Fine-tuning:** Extensive support for fine-tuning large language models (LLMs) and other generative models (Gemini, Gemma, Qwen, Llama, etc.) using various methods like PEFT and Axolotl, allowing models to adapt to specific domain data.\n",
    "* **Vertex AI Experiments:**\n",
    "    * **Purpose:** To track, compare, and analyze different model architectures, hyper-parameters, and training environments.\n",
    "    * **Role in MLOps:** Essential for experiment management, ensuring reproducibility and helping identify the best performing models.\n",
    "* **Vertex ML Metadata:**\n",
    "    * **Purpose:** To track the lineage of ML artifacts (datasets, models, metrics, parameters) and orchestrate ML workflows.\n",
    "    * **Role in MLOps:** Provides critical visibility into the entire ML lifecycle, enabling auditing, debugging, and understanding the dependencies between different components of an ML system.\n",
    "* **Generative AI Evaluation Service:**\n",
    "    * **Purpose:** Allows users to evaluate the quality and performance of generative models and applications against custom criteria.\n",
    "    * **Role in MLOps:** Crucial for validating the output of generative models (e.g., assessing factual accuracy, relevance, safety) before deployment and continuously monitoring their performance in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587027c5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. Model Deployment and Serving\n",
    "\n",
    "**Goal:** To deploy trained models into production environments for inference, ensuring high availability, low latency, and scalability.\n",
    "\n",
    "* **üßë‚Äçüéì - Vertex AI Endpoints:**\n",
    "    * **Purpose:** A fully managed service for deploying models for online (real-time) predictions.\n",
    "    * **Key Capabilities:**\n",
    "        * **Managed Prediction Service:** Handles infrastructure, scaling (auto-scaling), and updates, so users don't have to manage servers.\n",
    "        * **Low-latency Serving:** Optimized for real-time inference requests.\n",
    "        * **Model Versioning:** Allows deploying multiple versions of a model to the same endpoint for A/B testing or gradual rollouts.\n",
    "        * **Traffic Splitting:** Enables routing traffic to different model versions for A/B testing, canary deployments, or blue/green deployments.\n",
    "        * **Private Endpoints:** For secure, private access to models within your VPC network.\n",
    "* **üßë‚Äçüéì - Vertex AI Batch Prediction:**\n",
    "    * **Purpose:** For making predictions on large datasets asynchronously, where latency is not a critical factor.\n",
    "    * **Role in MLOps:** Ideal for generating insights on a schedule or for scenarios like reporting and large-scale data enrichment.\n",
    "* **üßë‚Äçüéì - Model Registry:**\n",
    "    * **Purpose:** A centralized repository for managing and versioning trained ML models.\n",
    "    * **Role in MLOps:** Stores metadata about models, enables model versioning, approval workflows, and facilitates the transition from training to deployment. From the Model Registry, you can evaluate models, deploy models to an endpoint, and create batch inferences.\n",
    "* **AI Hypercomputer:**\n",
    "    * **Purpose:** Google Cloud's integrated supercomputing system, leveraging Ironwood TPUs and NVIDIA GPUs.\n",
    "    * **Role in MLOps:** While primarily for training, its advanced compute capabilities are also leveraged for highly demanding inference workloads, especially for large generative models, ensuring efficient and fast serving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017df6e",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Model Monitoring & Governance\n",
    "\n",
    "**Goal:** To continuously monitor model performance in production, detect issues like data drift and concept drift, ensure fairness, and manage the model lifecycle.\n",
    "\n",
    "* **Vertex AI Model Monitoring:**\n",
    "    * **Purpose:** Monitors deployed models for drift (training-serving skew, inference drift), performance degradation, and anomalous feature distributions.\n",
    "    * **Key Capabilities:**\n",
    "        * **Drift Detection:** Automatically detects statistical differences between training data and serving data, or between historical and current inference data, which can indicate model decay.\n",
    "        * **Alerting:** Configurable alerts notify users of detected drift or performance issues, enabling timely intervention.\n",
    "        * **Feature Attribution Monitoring:** Helps understand which features are driving model predictions and if their importance changes over time.\n",
    "        * **Bias Detection:** Monitors for potential biases in model predictions.\n",
    "* **Vertex AI Pipelines:**\n",
    "    * **Purpose:** Automates, monitors, and governs your ML workflows in a serverless manner. It allows you to define ML workflows as a series of interconnected steps (pipeline tasks).\n",
    "    * **Key Capabilities (as of 2025):**\n",
    "        * **Workflow Orchestration:** Defines the entire MLOps lifecycle, from data ingestion and preprocessing to training, evaluation, deployment, and even retraining.\n",
    "        * **Reproducibility:** Ensures that ML workflows are repeatable and consistent by defining them as code.\n",
    "        * **Versioning:** Supports versioning of pipelines and components, enabling tracking of changes and rollbacks.\n",
    "        * **Scheduled Runs:** Automates pipeline execution on a schedule or in response to triggers (e.g., new data arrival).\n",
    "        * **Lineage Tracking:** Automatically tracks the lineage of artifacts within a pipeline run using Vertex ML Metadata, providing full traceability.\n",
    "        * **Multi-Modal Pipelines:** Enhanced support for orchestrating pipelines that process and generate multi-modal data (vision, text, audio, video) leveraging the latest Gemini models and other generative AI capabilities.\n",
    "        * **Enterprise Integration:** Seamless integration with BigQuery, Workspace, and other GCP services within pipeline components.\n",
    "* **Vertex AI Agent Engine and Agent Garden:**\n",
    "    * **Purpose:** While primarily for agent development, they play a crucial role in operationalizing and governing complex AI systems that might incorporate multiple models and decision-making logic. Agent Engine provides a managed runtime for deploying and securely managing AI agents, complete with memory management and evaluation tools.\n",
    "    * **Role in MLOps:** Enables the deployment and monitoring of intelligent agents that can automate complex tasks, potentially leveraging multiple ML models and decision points. This moves beyond just model serving to serving entire intelligent systems.\n",
    "* **Cloud Logging & Cloud Monitoring:**\n",
    "    * **Purpose:** General Google Cloud services for collecting logs and metrics from your applications and infrastructure.\n",
    "    * **Role in MLOps:** Provide foundational capabilities for monitoring the health and performance of your Vertex AI services, tracking resource utilization, and debugging issues.\n",
    "* **Identity and Access Management (IAM):**\n",
    "    * **Purpose:** Controls who can do what within your Google Cloud project.\n",
    "    * **Role in MLOps:** Essential for securing your ML pipelines, models, and data, ensuring proper access control and compliance.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45db5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fad93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
