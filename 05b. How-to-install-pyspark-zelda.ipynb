{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc59107-d827-44f1-a396-b115e6e4ef8c",
   "metadata": {},
   "source": [
    "\n",
    "**Why Vertex AI Workbench Doesn't Find PySpark by Default**\n",
    "\n",
    "  * **JupyterLab Environment:** A Vertex AI Workbench instance provides a JupyterLab environment running on a Compute Engine VM. While it comes with many data science libraries (TensorFlow, PyTorch, scikit-learn), it **doesn't inherently include a Spark distribution or PySpark configured to run locally or connect to a Spark cluster.**\n",
    "  * **Spark is a Distributed System:** PySpark is the Python API for Apache Spark, which is a *distributed computing framework*. It's designed to run across a cluster of machines. A single notebook instance, by itself, isn't a Spark cluster.\n",
    "  * **Missing Dependencies:** PySpark requires a Java Development Kit (JDK) and specific environment variables (`SPARK_HOME`, `JAVA_HOME`) to be set up correctly to find and interact with the underlying Spark binaries.\n",
    "\n",
    "**How to Enable PySpark in Vertex AI Workbench**\n",
    "\n",
    "There are two primary ways to get PySpark working in a Vertex AI Workbench notebook:\n",
    "\n",
    "**Method 1: Integrate with a Dataproc Cluster (Recommended for Big Data)**\n",
    "\n",
    "This is the most robust and scalable solution for real Big Data workloads. You connect your Workbench notebook to a managed Spark cluster (Dataproc).\n",
    "\n",
    "1.  **Enable Dataproc API:** Ensure the Dataproc API is enabled in your Google Cloud project.\n",
    "    ```bash\n",
    "    gcloud services enable dataproc.googleapis.com\n",
    "    ```\n",
    "2.  **Create a Dataproc Cluster:** Create a Dataproc cluster in the same region as your Workbench instance. You can do this via the GCP Console or `gcloud` commands.\n",
    "    ```bash\n",
    "\n",
    "    gcloud dataproc clusters create example-cluster\\\n",
    "      --enable-component-gateway\\\n",
    "      --bucket=example-dataproc-workshop\\\n",
    "      --region=europe-west1\\\n",
    "      --no-address\\\n",
    "      --master-machine-type=n4-standard-2\\\n",
    "      --master-boot-disk-type=hyperdisk-balanced\\\n",
    "      --master-boot-disk-size=100\\\n",
    "      --num-workers=2\\\n",
    "      --worker-machine-type=n4-standard-2\\\n",
    "      --worker-boot-disk-size=200\\\n",
    "      --image-version=2.2-debian12\\\n",
    "      --optional-components JUPYTER\\\n",
    "      --max-age=3600s\\\n",
    "      --labels=mode=workshop,user=zelda\\\n",
    "      --project=$PROJECT_ID\n",
    "    ```\n",
    "3.  **Enable Dataproc Integration in Workbench:**\n",
    "      * When creating a new Vertex AI Workbench instance, ensure \"Enable Dataproc Serverless Interactive Sessions\" (or \"Enable Dataproc\" for older versions) is selected under \"Advanced options\" -\\> \"Environment\" or \"Integrations.\"\n",
    "      * If your instance already exists, you might need to stop it, edit it, enable the Dataproc integration, and then restart it.\n",
    "4.  **Open JupyterLab and Select Dataproc Kernel:**\n",
    "      * Once your Workbench instance is running and has Dataproc integration enabled, open JupyterLab.\n",
    "      * When creating a new notebook, you should see kernels like \"PySpark\" or \"Spark (with Python 3)\" that allow you to connect to your Dataproc cluster.\n",
    "5.  **Connect to Spark Session in Notebook:** Your notebook code will then automatically connect to the Dataproc cluster when you create a SparkSession.\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"MyPySparkApp\").getOrCreate()\n",
    "    print(\"SparkSession created successfully!\")\n",
    "    ```\n",
    "\n",
    "**Method 2: Install PySpark Locally on the Workbench Instance (for Smaller Scale/Testing)**\n",
    "\n",
    "This method installs PySpark directly on your Workbench VM. It's suitable for smaller datasets that fit within the VM's memory and for local development/testing, but it won't leverage distributed computing.\n",
    "\n",
    "1.  **Open a Terminal in JupyterLab:** From your JupyterLab interface on the Workbench instance, go to `File` -\\> `New` -\\> `Terminal`.\n",
    "2.  **Install Java (if not present):** PySpark requires Java. Many default images have it, but if not:\n",
    "    ```bash\n",
    "    sudo apt-get update\n",
    "    sudo apt-get install default-jdk\n",
    "    ```\n",
    "3.  **Install PySpark via pip:**\n",
    "    ```bash\n",
    "    pip install pyspark\n",
    "    ```\n",
    "4.  **Set Environment Variables (Optional, but good practice):**\n",
    "    ```bash\n",
    "    # Add these to your ~/.bashrc or directly in the notebook cell\n",
    "    export JAVA_HOME=\"/usr/lib/jvm/default-java\" # Adjust if your JDK path is different\n",
    "    export SPARK_HOME=\"/opt/conda/lib/python3.11/site-packages/pyspark\" # Adjust based on your pip install location\n",
    "    export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "    ```\n",
    "      * You might need to restart the kernel or JupyterLab after setting these.\n",
    "5.  **Create SparkSession in Notebook:**\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"LocalPySpark\").getOrCreate()\n",
    "    print(\"Local SparkSession created successfully!\")\n",
    "    ```\n",
    "\n",
    "**Troubleshooting Tips:**\n",
    "\n",
    "  * **Check Java Installation:** Run `java -version` in the terminal to confirm Java is installed.\n",
    "  * **Environment Variables:** Ensure `JAVA_HOME` and `SPARK_HOME` are correctly set.\n",
    "  * **Kernel Selection:** Always choose the correct PySpark kernel in your JupyterLab notebook.\n",
    "  * **Restart Kernel:** After installing packages or changing environment variables, always restart the notebook kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b61cfc-0def-4f8a-be5f-f4026ef678e5",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f7e26-6ffb-4909-84c6-f26a12e8d0d2",
   "metadata": {},
   "source": [
    "Start by creating the cluster, you can launch this command here (use Python 3 kernel) or in a normal shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a81c6e-e705-4895-8ca0-8ec10da52f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on operation [projects/poc-example-ds/regions/europe-west1/operations/ac6f4b90-b421-3662-9386-5cf4e96453c6].\n",
      "Waiting for cluster creation operation...                                      \n",
      "\u001b[1;33mWARNING:\u001b[0m The firewall rules for specified network or subnetwork would allow ingress traffic from 0.0.0.0/0, which could be a security risk.\n",
      "Waiting for cluster creation operation...â ¶                                     "
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters create ale-cluster\\\n",
    "      --enable-component-gateway\\\n",
    "      --bucket=example-dataproc-workshop\\\n",
    "      --region=europe-west1\\\n",
    "      --no-address\\\n",
    "      --master-machine-type=n4-standard-2\\\n",
    "      --master-boot-disk-type=hyperdisk-balanced\\\n",
    "      --master-boot-disk-size=100\\\n",
    "      --num-workers=2\\\n",
    "      --worker-machine-type=n4-standard-2\\\n",
    "      --worker-boot-disk-size=200\\\n",
    "      --image-version=2.2-debian12\\\n",
    "      --optional-components JUPYTER\\\n",
    "      --max-age=3600s\\\n",
    "      --labels=mode=workshop,user=zelda\\\n",
    "      --project=poc-example-ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8247e-7513-43dc-b6ec-5da082c21e09",
   "metadata": {},
   "source": [
    "Now select the kernel again (upper right corner) and set it to pyspark \"CLUSTER_NAME\" in our case example-cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8728417-16ea-4022-84c7-675e0b6f8202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BQ_INPUT_TABLE_ID = f\"poc-example-ds.example_composer_workshop.sales_data\" # Replace with your input table\n",
    "BQ_OUTPUT_TABLE_ID = f\"poc-example-ds.example_composer_workshop.filtered_sales\" # Replace with your output table\n",
    "INPUT_DATE='2025-06-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0807840f-51eb-4951-afcd-ddd7316028ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully!\n",
      "Spark UI: http://zelda-cluster-m.europe-west1-b.c.poc-example-ds.internal:36369\n",
      "Spark Version: 3.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 10:12:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Get or create a SparkSession.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BasicPySparkOperations\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.38.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bcbc9b-7369-4b6d-a0b9-9a0410fe25ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 10:12:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"EBClientZeldaProcessor\") \\\n",
    "        .config(\"temporaryGcsBucket\", \"example-dataproc-workshop\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665a6e36-56f4-45fc-a6d0-aab8599c0c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " df = spark.read.format(\"bigquery\") \\\n",
    "            .option(\"table\", BQ_INPUT_TABLE_ID) \\\n",
    "            .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99529de-f210-4c52-83a1-ee7214d118a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+----------------+\n",
      "|transaction_id|product_id|amount|transaction_date|\n",
      "+--------------+----------+------+----------------+\n",
      "|        TID003|    PROD_A|  75.2|      2025-06-11|\n",
      "|        TID001|    PROD_A|150.75|      2025-06-12|\n",
      "|        TID006|    PROD_A| 99.99|      2025-06-12|\n",
      "|        TID002|    PROD_B| 200.0|      2025-06-12|\n",
      "|        TID005|    PROD_B|120.99|      2025-06-13|\n",
      "|        TID004|    PROD_C| 300.5|      2025-06-12|\n",
      "|        TID008|    PROD_C| 450.0|      2025-06-13|\n",
      "|        TID007|    PROD_D|  50.0|      2025-06-11|\n",
      "+--------------+----------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2faf8110-3065-41d6-bf2d-55dd972b6d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processing_date=\"2025-06-12\"\n",
    "\n",
    "# Example processing: Filter by date and calculate total sales\n",
    "processed_df = df.filter(F.col(\"transaction_date\") == F.lit(processing_date)) \\\n",
    "                 .groupBy(\"product_id\") \\\n",
    "                 .agg(F.sum(\"amount\").alias(\"total_sales\")) \\\n",
    "                 .withColumn(\"processing_date\", F.lit(processing_date))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e39796-5072-4e9e-8aa5-e754cdfeaf72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing processed data to BigQuery table: poc-example-ds.example_composer_workshop.filtered_sales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Example of using a utility function\n",
    "print(f\"Writing processed data to BigQuery table: {BQ_OUTPUT_TABLE_ID}\")\n",
    "\n",
    "# Write to BigQuery\n",
    "processed_df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", BQ_OUTPUT_TABLE_ID) \\\n",
    "    .mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96ae6e-dde9-4241-bdc5-fffea7539d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
