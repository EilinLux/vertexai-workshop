{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc59107-d827-44f1-a396-b115e6e4ef8c",
   "metadata": {},
   "source": [
    "\n",
    "**Why Vertex AI Workbench Doesn't Find PySpark by Default**\n",
    "\n",
    "  * **JupyterLab Environment:** A Vertex AI Workbench instance provides a JupyterLab environment running on a Compute Engine VM. While it comes with many data science libraries (TensorFlow, PyTorch, scikit-learn), it **doesn't inherently include a Spark distribution or PySpark configured to run locally or connect to a Spark cluster.**\n",
    "  * **Spark is a Distributed System:** PySpark is the Python API for Apache Spark, which is a *distributed computing framework*. It's designed to run across a cluster of machines. A single notebook instance, by itself, isn't a Spark cluster.\n",
    "  * **Missing Dependencies:** PySpark requires a Java Development Kit (JDK) and specific environment variables (`SPARK_HOME`, `JAVA_HOME`) to be set up correctly to find and interact with the underlying Spark binaries.\n",
    "\n",
    "**How to Enable PySpark in Vertex AI Workbench**\n",
    "\n",
    "There are two primary ways to get PySpark working in a Vertex AI Workbench notebook:\n",
    "\n",
    "**Method 1: Integrate with a Dataproc Cluster (Recommended for Big Data)**\n",
    "\n",
    "This is the most robust and scalable solution for real Big Data workloads. You connect your Workbench notebook to a managed Spark cluster (Dataproc).\n",
    "\n",
    "1.  **Enable Dataproc API:** Ensure the Dataproc API is enabled in your Google Cloud project.\n",
    "    ```bash\n",
    "    gcloud services enable dataproc.googleapis.com\n",
    "    ```\n",
    "2.  **Create a Dataproc Cluster:** Create a Dataproc cluster in the same region as your Workbench instance. You can do this via the GCP Console or `gcloud` commands.\n",
    "    ```bash\n",
    "\n",
    "    gcloud dataproc clusters create example-cluster\\\n",
    "      --enable-component-gateway\\\n",
    "      --bucket=example-dataproc-workshop\\\n",
    "      --region=europe-west1\\\n",
    "      --no-address\\\n",
    "      --master-machine-type=n4-standard-2\\\n",
    "      --master-boot-disk-type=hyperdisk-balanced\\\n",
    "      --master-boot-disk-size=100\\\n",
    "      --num-workers=2\\\n",
    "      --worker-machine-type=n4-standard-2\\\n",
    "      --worker-boot-disk-size=200\\\n",
    "      --image-version=2.2-debian12\\\n",
    "      --optional-components JUPYTER\\\n",
    "      --max-age=3600s\\\n",
    "      --labels=mode=workshop,user=zelda\\\n",
    "      --project=$PROJECT_ID\n",
    "    ```\n",
    "3.  **Enable Dataproc Integration in Workbench:**\n",
    "      * When creating a new Vertex AI Workbench instance, ensure \"Enable Dataproc Serverless Interactive Sessions\" (or \"Enable Dataproc\" for older versions) is selected under \"Advanced options\" -\\> \"Environment\" or \"Integrations.\"\n",
    "      * If your instance already exists, you might need to stop it, edit it, enable the Dataproc integration, and then restart it.\n",
    "4.  **Open JupyterLab and Select Dataproc Kernel:**\n",
    "      * Once your Workbench instance is running and has Dataproc integration enabled, open JupyterLab.\n",
    "      * When creating a new notebook, you should see kernels like \"PySpark\" or \"Spark (with Python 3)\" that allow you to connect to your Dataproc cluster.\n",
    "5.  **Connect to Spark Session in Notebook:** Your notebook code will then automatically connect to the Dataproc cluster when you create a SparkSession.\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"MyPySparkApp\").getOrCreate()\n",
    "    print(\"SparkSession created successfully!\")\n",
    "    ```\n",
    "\n",
    "**Method 2: Install PySpark Locally on the Workbench Instance (for Smaller Scale/Testing)**\n",
    "\n",
    "This method installs PySpark directly on your Workbench VM. It's suitable for smaller datasets that fit within the VM's memory and for local development/testing, but it won't leverage distributed computing.\n",
    "\n",
    "1.  **Open a Terminal in JupyterLab:** From your JupyterLab interface on the Workbench instance, go to `File` -\\> `New` -\\> `Terminal`.\n",
    "2.  **Install Java (if not present):** PySpark requires Java. Many default images have it, but if not:\n",
    "    ```bash\n",
    "    sudo apt-get update\n",
    "    sudo apt-get install default-jdk\n",
    "    ```\n",
    "3.  **Install PySpark via pip:**\n",
    "    ```bash\n",
    "    pip install pyspark\n",
    "    ```\n",
    "4.  **Set Environment Variables (Optional, but good practice):**\n",
    "    ```bash\n",
    "    # Add these to your ~/.bashrc or directly in the notebook cell\n",
    "    export JAVA_HOME=\"/usr/lib/jvm/default-java\" # Adjust if your JDK path is different\n",
    "    export SPARK_HOME=\"/opt/conda/lib/python3.11/site-packages/pyspark\" # Adjust based on your pip install location\n",
    "    export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "    ```\n",
    "      * You might need to restart the kernel or JupyterLab after setting these.\n",
    "5.  **Create SparkSession in Notebook:**\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"LocalPySpark\").getOrCreate()\n",
    "    print(\"Local SparkSession created successfully!\")\n",
    "    ```\n",
    "\n",
    "**Troubleshooting Tips:**\n",
    "\n",
    "  * **Check Java Installation:** Run `java -version` in the terminal to confirm Java is installed.\n",
    "  * **Environment Variables:** Ensure `JAVA_HOME` and `SPARK_HOME` are correctly set.\n",
    "  * **Kernel Selection:** Always choose the correct PySpark kernel in your JupyterLab notebook.\n",
    "  * **Restart Kernel:** After installing packages or changing environment variables, always restart the notebook kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b61cfc-0def-4f8a-be5f-f4026ef678e5",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f7e26-6ffb-4909-84c6-f26a12e8d0d2",
   "metadata": {},
   "source": [
    "Start by creating the cluster, you can launch this command here (use Python 3 kernel) or in a normal shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a81c6e-e705-4895-8ca0-8ec10da52f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on operation [projects/poc-example-ds/regions/europe-west1/operations/ca1b0bbe-23d8-3def-86f8-4918d4fbf636].\n",
      "Waiting for cluster creation operation...                                      \n",
      "\u001b[1;33mWARNING:\u001b[0m The firewall rules for specified network or subnetwork would allow ingress traffic from 0.0.0.0/0, which could be a security risk.\n",
      "Waiting for cluster creation operation...done.                                 \n",
      "Created [https://dataproc.googleapis.com/v1/projects/poc-example-ds/regions/europe-west1/clusters/example-cluster] Cluster placed in zone [europe-west1-b].\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters create example-cluster\\\n",
    "      --enable-component-gateway\\\n",
    "      --bucket=example-dataproc-workshop\\\n",
    "      --region=europe-west1\\\n",
    "      --no-address\\\n",
    "      --master-machine-type=n4-standard-2\\\n",
    "      --master-boot-disk-type=hyperdisk-balanced\\\n",
    "      --master-boot-disk-size=100\\\n",
    "      --num-workers=2\\\n",
    "      --worker-machine-type=n4-standard-2\\\n",
    "      --worker-boot-disk-size=200\\\n",
    "      --image-version=2.2-debian12\\\n",
    "      --optional-components JUPYTER\\\n",
    "      --max-age=3600s\\\n",
    "      --labels=mode=workshop,user=zelda\\\n",
    "      --project=poc-example-ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8247e-7513-43dc-b6ec-5da082c21e09",
   "metadata": {},
   "source": [
    "Now select the kernel again (upper right corner) and set it to pyspark \"CLUSTER_NAME\" in our case example-cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0807840f-51eb-4951-afcd-ddd7316028ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully!\n",
      "Spark UI: http://example-cluster-m.europe-west1-b.c.poc-example-ds.internal:39695\n",
      "Spark Version: 3.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 14:24:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Get or create a SparkSession.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BasicPySparkOperations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7e3918-cece-470e-8265-5a3f259316ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 14:24:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: gs://example-dataproc-workshop/01_2_Submit_Job/raw_data/sales/\n",
      "Processing data for date: 2025-06-12\n",
      "Writing processed data to BigQuery table: example_dataproc_workshop.processed_sales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 14:24:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:25:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:25:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:25:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:25:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:26:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:26:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:26:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:26:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:27:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:27:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:27:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:27:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:28:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:28:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:28:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:28:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:29:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:29:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:29:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:29:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:30:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:30:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:30:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:30:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:31:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:31:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:31:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:31:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:32:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:32:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:32:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:32:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:33:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:33:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:33:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:33:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:34:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:34:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:34:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:34:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:35:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:35:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:35:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:35:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:36:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:36:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:36:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:36:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:37:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:37:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:37:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:37:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:38:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 14:38:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "# process_data.py\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "\n",
    "\n",
    "def main(input_path, processing_date, output_table):\n",
    "\n",
    " \n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SalesDataProcessor\") \\\n",
    "        .config(\"temporaryGcsBucket\", \"example-dataproc-workshop\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Define schema for input data (adjust as per your actual data)\n",
    "    input_schema = StructType([\n",
    "        StructField(\"transaction_id\", StringType(), True),\n",
    "        StructField(\"product_id\", StringType(), True),\n",
    "        StructField(\"amount\", DoubleType(), True),\n",
    "        StructField(\"transaction_date\", DateType(), True)\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        print(f\"Reading data from: {input_path}\")\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .schema(input_schema) \\\n",
    "            .csv(input_path)\n",
    "\n",
    "        print(f\"Processing data for date: {processing_date}\")\n",
    "        # Example processing: Filter by date and calculate total sales\n",
    "        processed_df = df.filter(F.col(\"transaction_date\") == F.lit(processing_date)) \\\n",
    "                         .groupBy(\"product_id\") \\\n",
    "                         .agg(F.sum(\"amount\").alias(\"total_sales\")) \\\n",
    "                         .withColumn(\"processing_date\", F.lit(processing_date))\n",
    "\n",
    "        # Example of using a utility function\n",
    "        # processed_df = some_utility_function(processed_df)\n",
    "\n",
    "        print(f\"Writing processed data to BigQuery table: {output_table}\")\n",
    "\n",
    "\n",
    "        # Write to BigQuery\n",
    "        processed_df.write \\\n",
    "            .format(\"bigquery\") \\\n",
    "            .option(\"table\", output_table) \\\n",
    "            .mode(\"append\").save()\n",
    "        # mode can be  \"append\" or \"overwrite\" or \"ignore\" or \"errorifexists\"\n",
    "        print(\"PySpark job completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        spark.stop()\n",
    "        raise # Re-raise the exception to indicate job failure\n",
    "\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(input_path=\"gs://example-dataproc-workshop/01_2_Submit_Job/raw_data/sales/\",\n",
    "     output_table=\"example_dataproc_workshop.processed_sales\",\n",
    "    processing_date=\"2025-06-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe44e88-3df1-4e22-b087-571ed3cff3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
