{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c375c4f-df80-4a70-997e-cbc3d709817d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 13:52:54.445494: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-28 13:52:55.731298: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-07-28 13:52:55.731426: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-07-28 13:52:55.731437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2025-07-28 13:52:56,777 - INFO - Starting TensorFlow basic test on Vertex AI Workbench.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "# --- Configure Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Starting TensorFlow basic test on Vertex AI Workbench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f061ebaf-7024-476f-944f-9021241eedad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 13:53:02,051 - INFO - TensorFlow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Check TensorFlow Version ---\n",
    "tf_version = tf.__version__\n",
    "logger.info(f\"TensorFlow version: {tf_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38613d1a-55f4-4685-9bf0-35e650d22360",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 13:53:07,021 - WARNING - No GPUs found. TensorFlow will run on CPU.\n",
      "2025-07-28 13:53:07.021229: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-07-28 13:53:07.021273: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-07-28 13:53:07.021299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (spark-workbench): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Check GPU Availability ---\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    logger.info(f\"GPUs available: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        logger.info(f\"  - {gpu}\")\n",
    "    # Set TensorFlow to use GPU memory growth to avoid allocating all memory at once\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logger.info(\"GPU memory growth set to True.\")\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Error setting GPU memory growth: {e}\")\n",
    "else:\n",
    "    logger.warning(\"No GPUs found. TensorFlow will run on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ffa0eb-c0a8-4efe-b56f-1e102061865c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 13:53:26.890799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-28 13:53:26,946 - INFO - Tensor 'a':\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "2025-07-28 13:53:26,948 - INFO - Tensor 'b':\n",
      "[[5. 6.]\n",
      " [7. 8.]]\n",
      "2025-07-28 13:53:26,950 - INFO - Result of matrix multiplication (c):\n",
      "[[19. 22.]\n",
      " [43. 50.]]\n",
      "2025-07-28 13:53:26,951 - INFO - Simple matrix multiplication test PASSED.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Perform a Simple TensorFlow Operation (CPU/GPU agnostic) ---\n",
    "try:\n",
    "    # Create two constant tensors\n",
    "    a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "    b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "    # Perform matrix multiplication\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "    logger.info(f\"Tensor 'a':\\n{a.numpy()}\")\n",
    "    logger.info(f\"Tensor 'b':\\n{b.numpy()}\")\n",
    "    logger.info(f\"Result of matrix multiplication (c):\\n{c.numpy()}\")\n",
    "\n",
    "    # Verify a simple calculation\n",
    "    expected_c = np.array([[19., 22.], [43., 50.]])\n",
    "    if np.array_equal(c.numpy(), expected_c):\n",
    "        logger.info(\"Simple matrix multiplication test PASSED.\")\n",
    "    else:\n",
    "        logger.error(\"Simple matrix multiplication test FAILED: Unexpected result.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"An error occurred during TensorFlow operation:\", exc_info=True)\n",
    "    logger.error(\"TensorFlow basic test FAILED.\")\n",
    "    exit(1) # Exit with an error code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281f21ff-f394-423e-b903-a6997ca03954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 13:53:37,069 - INFO - Model: \"sequential\"\n",
      "2025-07-28 13:53:37,070 - INFO - _________________________________________________________________\n",
      "2025-07-28 13:53:37,071 - INFO -  Layer (type)                Output Shape              Param #   \n",
      "2025-07-28 13:53:37,072 - INFO - =================================================================\n",
      "2025-07-28 13:53:37,073 - INFO -  dense (Dense)               (None, 10)                60        \n",
      "2025-07-28 13:53:37,074 - INFO -                                                                  \n",
      "2025-07-28 13:53:37,075 - INFO -  dense_1 (Dense)             (None, 1)                 11        \n",
      "2025-07-28 13:53:37,076 - INFO -                                                                  \n",
      "2025-07-28 13:53:37,077 - INFO - =================================================================\n",
      "2025-07-28 13:53:37,078 - INFO - Total params: 71\n",
      "2025-07-28 13:53:37,079 - INFO - Trainable params: 71\n",
      "2025-07-28 13:53:37,080 - INFO - Non-trainable params: 0\n",
      "2025-07-28 13:53:37,081 - INFO - _________________________________________________________________\n",
      "2025-07-28 13:53:37,081 - INFO - Keras Sequential model defined and compiled successfully.\n",
      "2025-07-28 13:53:37,082 - INFO - TensorFlow basic test completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Simple Model Definition and Summary (Optional, but good for ML setup) ---\n",
    "try:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10, activation='relu', input_shape=(5,)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.summary(print_fn=logger.info) # Print summary to logs\n",
    "    logger.info(\"Keras Sequential model defined and compiled successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error defining Keras model: {e}\")\n",
    "    logger.error(\"TensorFlow Keras model test FAILED.\")\n",
    "\n",
    "logger.info(\"TensorFlow basic test completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede53740-a718-4e5f-9be2-d56eb11c5a5d",
   "metadata": {},
   "source": [
    "# Classes and methods definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8835553-5f56-47b7-8788-f862c7bcb625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.config.threading.set_intra_op_parallelism_threads(14)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e354d5ec-cc9d-4900-9fa3-231ca2d23ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def elu_plus_one_plus_epsilon(x):\n",
    "    return tf.keras.activations.elu(x) + 1 + tf.keras.backend.epsilon()\n",
    "\n",
    "class MixtureDensityOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dimension, num_mixtures, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dimension\n",
    "        self.num_mix = num_mixtures\n",
    "        self.mdn_mus = tf.keras.layers.Dense(\n",
    "            self.num_mix * self.output_dim, name=\"mdn_mus\"\n",
    "        )  # mix*output vals, no activation\n",
    "        self.mdn_sigmas = tf.keras.layers.Dense(\n",
    "            self.num_mix * self.output_dim,\n",
    "            activation=elu_plus_one_plus_epsilon,\n",
    "            name=\"mdn_sigmas\",\n",
    "        )  # mix*output vals exp activation\n",
    "        self.mdn_pi = tf.keras.layers.Dense(self.num_mix, name=\"mdn_pi\")  # mix vals, logits\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mdn_mus.build(input_shape)\n",
    "        self.mdn_sigmas.build(input_shape)\n",
    "        self.mdn_pi.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        return (\n",
    "            self.mdn_mus.trainable_weights\n",
    "            + self.mdn_sigmas.trainable_weights\n",
    "            + self.mdn_pi.trainable_weights\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def non_trainable_weights(self):\n",
    "        return (\n",
    "            self.mdn_mus.non_trainable_weights\n",
    "            + self.mdn_sigmas.non_trainable_weights\n",
    "            + self.mdn_pi.non_trainable_weights\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return tf.keras.layers.concatenate(\n",
    "            [self.mdn_mus(x), self.mdn_sigmas(x), self.mdn_pi(x)], name=\"mdn_outputs\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24917ea-0bc6-41cf-8b38-e8c5ba0b6f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MDNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_mixtures, output_dim):\n",
    "        super(MDNLayer, self).__init__()\n",
    "        self.num_mixtures = num_mixtures\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Layer to predict mixture weights (softmax for probabilities)\n",
    "        self.dense_weights = tf.keras.layers.Dense(num_mixtures, activation='softmax')\n",
    "        \n",
    "        # Layer to predict means for each mixture component\n",
    "        self.dense_means = tf.keras.layers.Dense(num_mixtures * output_dim)\n",
    "        \n",
    "        # Layer to predict standard deviations for each mixture component\n",
    "        self.dense_stds = tf.keras.layers.Dense(num_mixtures * output_dim, activation='softplus')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Predict the mixture parameters\n",
    "        # weights = tf.nn.log_softmax(self.dense_weights(inputs))\n",
    "        weights = self.dense_weights(inputs)\n",
    "        means = self.dense_means(inputs)\n",
    "        stds = self.dense_stds(inputs)\n",
    "        \n",
    "        # Reshape means and stds to match the number of mixtures and output dimension\n",
    "        means = tf.reshape(means, (-1, self.num_mixtures, self.output_dim))\n",
    "        stds = tf.reshape(stds, (-1, self.num_mixtures, self.output_dim))\n",
    "        \n",
    "        return weights, means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02cf4c65-336c-4831-99cd-a16948df43c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lstm_model(timesteps, num_features, num_output_features, num_mixtures, units=64, output_steps=14, conv_kernel=3):\n",
    "    LSTM_model = tf.keras.Sequential()\n",
    "    LSTM_model.add(tf.keras.layers.Input((timesteps, num_features)))\n",
    "    LSTM_model.add(tf.keras.layers.BatchNormalization())\n",
    "    LSTM_model.add(tf.keras.layers.Conv1D(filters=units, kernel_size=conv_kernel, strides=1, padding='same'))\n",
    "    # LSTM_model.add(tf.keras.layers.Dropout(0.2))\n",
    "    LSTM_model.add(tf.keras.layers.BatchNormalization())\n",
    "    LSTM_model.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "    LSTM_model.add(tf.keras.layers.BatchNormalization())\n",
    "    # LSTM_model.add(tf.keras.layers.Dropout(0.2))\n",
    "    LSTM_model.add(tf.keras.layers.LSTM(units=int(units/2)))\n",
    "    LSTM_model.add(tf.keras.layers.BatchNormalization())\n",
    "    # LSTM_model.add(tf.keras.layers.Dropout(0.2))\n",
    "    LSTM_model.add(MixtureDensityOutput(num_output_features, num_mixtures))\n",
    "    LSTM_model.add(tf.keras.layers.BatchNormalization())\n",
    "    # LSTM_model.add((tf.keras.layers.Dense(int(units/4), activation='relu')))\n",
    "    # LSTM_model.add(tf.keras.layers.BatchNormalization())\n",
    "    LSTM_model.add((tf.keras.layers.Dense(num_output_features, activation='sigmoid')))\n",
    "\n",
    "    return LSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad4fb28-5b6f-43db-bade-61c6761be255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedBack(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, units, \n",
    "                 out_steps, \n",
    "                 num_features, \n",
    "                 timesteps, \n",
    "                 num_output_features=1, \n",
    "                 num_mixtures=5, \n",
    "                 stochastic=True, \n",
    "                 feature_extraction=True,\n",
    "                 kernel=3):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_output_features = num_output_features\n",
    "        self.num_mixtures = num_mixtures\n",
    "        self.num_features = num_features\n",
    "        self.timesteps = timesteps\n",
    "        self.out_steps = out_steps\n",
    "        self.units = units\n",
    "        self.stochastic = stochastic\n",
    "        self.conv_filters = units\n",
    "        self.feature_extraction = feature_extraction\n",
    "        \n",
    "        self.cnn_layer = tf.keras.layers.Conv1D(\n",
    "            filters=self.conv_filters,            # Number of filters (reduce feature space)\n",
    "            kernel_size=kernel,         # Kernel size (adjust based on input characteristics)\n",
    "            strides=1,             # Stride size (keep it 1 to preserve sequence length)\n",
    "            padding='same',        # Keep the timesteps dimension unchanged\n",
    "            activation='relu'      # Non-linear activation function\n",
    "        )\n",
    "        self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
    "        # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "        self.lstm_rnn = tf.keras.layers.RNN(\n",
    "            self.lstm_cell, return_state=True, unroll=True\n",
    "        )\n",
    "        # self.dense = tf.keras.layers.Dense(num_output_features, activation=\"sigmoid\")\n",
    "        # self.output_layer = tf.keras.layers.Dense(num_output_features, activation=None)\n",
    "        self.mdn_layer = MDNLayer(self.num_mixtures, num_output_features)\n",
    "    \n",
    "    def warmup(self, inputs):\n",
    "        # inputs.shape :> (batch, time, features)\n",
    "        # x.shape :> (batch, lstm_units)\n",
    "        x, *state = self.lstm_rnn(inputs)\n",
    "        # predictions.shape => (batch, features)\n",
    "        # prediction = self.dense(x)\n",
    "        # print(x.shape)\n",
    "        weights, means, stds = self.mdn_layer(x)\n",
    "        # prediction = self.sample_mixture(weights, means, stds)\n",
    "        # print(prediction.shape)\n",
    "        return weights, means, stds, state\n",
    "    \n",
    "    def sample_mixture(self, weights, means, stds, training):\n",
    "        \"\"\"Sample from the mixture of Gaussians\"\"\"\n",
    "        # Sample a component index based on mixture weights\n",
    "\n",
    "        if self.stochastic:\n",
    "            component_idx = tf.random.categorical(tf.math.log(weights), 1)\n",
    "            component_idx = tf.squeeze(component_idx, axis=-1)  # shape (batch,)\n",
    "        else:\n",
    "            component_idx = tf.argmax(weights, axis=-1)\n",
    "            # component_idx = tf.expand_dims(component_idx, axis=-1)  # Ensure the correct shape\n",
    "        \n",
    "        # Gather the means and stds based on the sampled component index\n",
    "        selected_means = tf.gather(means, component_idx, batch_dims=1)  # shape (batch, output_dim)\n",
    "        selected_stds = tf.gather(stds, component_idx, batch_dims=1)  # shape (batch, output_dim)\n",
    "        \n",
    "        # Sample from the selected Gaussian (Normal distribution)\n",
    "        if self.stochastic:\n",
    "            sampled_values = selected_means + selected_stds * tf.random.normal(tf.shape(selected_means))\n",
    "        else:\n",
    "            sampled_values = selected_means\n",
    "        \n",
    "        return sampled_values\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "        all_predictions = []\n",
    "        if self.feature_extraction:\n",
    "            inputs = self.cnn_layer(inputs)\n",
    "        # Initialize the LSTM state.\n",
    "        weights, means, stds, state = self.warmup(inputs)\n",
    "        weights_reshaped = tf.expand_dims(weights, axis=-1)\n",
    "        # print('weights reshaped', weights_reshaped.shape)\n",
    "        # print('weights', weights.shape)\n",
    "        # print('means', means.shape)\n",
    "        # print('stds', stds.shape)\n",
    "        # Insert the first prediction.\n",
    "        all_predictions.append(tf.concat([weights_reshaped, means, stds], axis=1))\n",
    "\n",
    "        # Run the rest of the prediction steps.\n",
    "        for n in range(1, self.out_steps):\n",
    "            # Use the last prediction as input.\n",
    "            x = self.sample_mixture(weights, means, stds, training)\n",
    "            x = tf.repeat(x, self.units, axis=-1)\n",
    "            # Execute one lstm step.\n",
    "            x, state = self.lstm_cell(x, states=state, training=training)\n",
    "            # Convert the lstm output to a prediction.\n",
    "            weights, means, stds = self.mdn_layer(x)\n",
    "            weights_reshaped = tf.expand_dims(weights, axis=-1)\n",
    "            # print('weights', weights_reshaped.shape)\n",
    "            # print('means', means.shape)\n",
    "            # print('stds', stds.shape)\n",
    "            # prediction = self.sample_mixture(weights, means, stds)\n",
    "            # Add the prediction to the output.\n",
    "            all_predictions.append(tf.concat([weights_reshaped, means, stds], axis=1))\n",
    "\n",
    "        # predictions.shape => (time, batch, features)\n",
    "        # print('concat', all_predictions[0].shape)\n",
    "        predictions = tf.concat(all_predictions, axis=-1)\n",
    "        # print('stack', predictions.shape)\n",
    "        # predictions.shape => (batch, time, features)\n",
    "        predictions = tf.transpose(predictions, [0, 2, 1])\n",
    "        # print('transpose', predictions.shape)\n",
    "        return predictions\n",
    "    \n",
    "    def load_weights_and_build(self, weights_path):\n",
    "        self.build(input_shape=(None, self.timesteps, self.num_features))\n",
    "        self.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d390aa-1edb-4146-88d0-35c3e1ef3124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedBackAttetion(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, units, out_steps, num_features, timesteps, num_output_features=1, num_mixtures=5, stochastic=True, feature_extraction=True, attention_heads=3):\n",
    "        super().__init__()\n",
    "        self.num_output_features = num_output_features\n",
    "        self.num_mixtures = num_mixtures\n",
    "        self.num_features = num_features\n",
    "        self.timesteps = timesteps\n",
    "        self.out_steps = out_steps\n",
    "        self.units = units\n",
    "        self.stochastic = stochastic\n",
    "        self.conv_filters = units\n",
    "        self.feature_extraction = feature_extraction\n",
    "        self.attention_heads = attention_heads\n",
    "        \n",
    "        # self.cnn_dropout = tf.keras.layers.Dropout(0.6)\n",
    "        self.cnn_layer = tf.keras.layers.Conv1D(\n",
    "            filters=self.conv_filters,            # Number of filters (reduce feature space)\n",
    "            kernel_size=3,         # Kernel size (adjust based on input characteristics)\n",
    "            strides=1,             # Stride size (keep it 1 to preserve sequence length)\n",
    "            padding='same',        # Keep the timesteps dimension unchanged\n",
    "            activation='relu',      # Non-linear activation function\n",
    "            kernel_initializer='zeros',\n",
    "            # kernel_regularizer=tf.keras.regularizers.L2()\n",
    "        )\n",
    "        self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
    "        # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "        self.lstm_rnn = tf.keras.layers.RNN(\n",
    "            self.lstm_cell, return_state=True, unroll=True\n",
    "        )\n",
    "\n",
    "        self.attention_layer = tf.keras.layers.MultiHeadAttention(num_heads=self.attention_heads, \n",
    "                                                                  key_dim=self.out_steps,\n",
    "                                                                  kernel_initializer='zeros',\n",
    "                                                                #   kernel_regularizer=tf.keras.regularizers.L2(),\n",
    "                                                                #   dropout=0.6\n",
    "                                                                  )\n",
    "\n",
    "        # self.dense = tf.keras.layers.Dense(num_output_features, activation=\"sigmoid\")\n",
    "        # self.output_layer = tf.keras.layers.Dense(num_output_features, activation=None)\n",
    "        self.mdn_layer = MDNLayer(self.num_mixtures, num_output_features)\n",
    "    \n",
    "    def warmup(self, inputs):\n",
    "        # inputs.shape => (batch, time, features)\n",
    "        # x.shape => (batch, lstm_units)\n",
    "        # print(\"input\", inputs.shape)\n",
    "        x, *state = self.lstm_rnn(inputs)\n",
    "        # predictions.shape => (batch, features)\n",
    "        # prediction = self.dense(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        # print(\"expand\", x.shape)\n",
    "        x = self.attention_layer(query=x, value=x, key=x)\n",
    "        # print(\"attention\", x.shape)\n",
    "        weights, means, stds = self.mdn_layer(x)\n",
    "        # prediction = self.sample_mixture(weights, means, stds)\n",
    "        # print(prediction.shape)\n",
    "        return weights, means, stds, state\n",
    "    \n",
    "    def sample_mixture(self, weights, means, stds, training):\n",
    "        \"\"\"Sample from the mixture of Gaussians\"\"\"\n",
    "        # Sample a component index based on mixture weights\n",
    "\n",
    "        if training == True and self.stochastic:\n",
    "            component_idx = tf.random.categorical(tf.math.log(weights), 1)\n",
    "            component_idx = tf.squeeze(component_idx, axis=-1)  # shape (batch,)\n",
    "        else:\n",
    "            component_idx = tf.argmax(weights, axis=-1)\n",
    "            # component_idx = tf.expand_dims(component_idx, axis=-1)  # Ensure the correct shape\n",
    "        \n",
    "        # Gather the means and stds based on the sampled component index\n",
    "        selected_means = tf.gather(means, component_idx, batch_dims=1)  # shape (batch, output_dim)\n",
    "        selected_stds = tf.gather(stds, component_idx, batch_dims=1)  # shape (batch, output_dim)\n",
    "        \n",
    "        # Sample from the selected Gaussian (Normal distribution)\n",
    "        if training == True and self.stochastic:\n",
    "            sampled_values = selected_means + selected_stds * tf.random.normal(tf.shape(selected_means))\n",
    "        else:\n",
    "            sampled_values = selected_means\n",
    "        \n",
    "        return sampled_values\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "        all_predictions = []\n",
    "        if self.feature_extraction:\n",
    "            inputs = self.cnn_layer(inputs)\n",
    "            # inputs = self.cnn_dropout(inputs)\n",
    "        # Initialize the LSTM state.\n",
    "        weights, means, stds, state = self.warmup(inputs)\n",
    "        weights_reshaped = tf.reshape(weights, (-1, weights.shape[-1], 1))\n",
    "        # weights_reshaped = tf.expand_dims(weights, axis=-1)\n",
    "        # print('weights', weights.shape)\n",
    "        # print('weightsreshape', weights_reshaped.shape)\n",
    "        # print('means', means.shape)\n",
    "        # print('stds', stds.shape)\n",
    "        # Insert the first prediction.\n",
    "        all_predictions.append(tf.concat([weights_reshaped, means, stds], axis=1))\n",
    "\n",
    "        # Run the rest of the prediction steps.\n",
    "        for n in range(1, self.out_steps):\n",
    "            # Use the last prediction as input.\n",
    "            x = self.sample_mixture(tf.reshape(weights, (-1, weights.shape[-1])), means, stds, training)\n",
    "            x = tf.repeat(x, self.units, axis=-1)\n",
    "            # Execute one lstm step.\n",
    "            x = tf.reshape(x, (-1, x.shape[-1]))\n",
    "            # print(\"cell\", x.shape)\n",
    "            x, state = self.lstm_cell(x, states=state, training=training)\n",
    "            # Convert the lstm output to a prediction.\n",
    "            x = tf.expand_dims(x, axis=1)\n",
    "            x = self.attention_layer(query=x, value=x, key=x)\n",
    "            weights, means, stds = self.mdn_layer(x)\n",
    "            # weights_reshaped = tf.expand_dims(weights, axis=-1)\n",
    "            # print('weights', weights_reshaped.shape)\n",
    "            # print('means', means.shape)\n",
    "            # print('stds', stds.shape)\n",
    "            # prediction = self.sample_mixture(weights, means, stds)\n",
    "            # Add the prediction to the output.\n",
    "            weights_reshaped = tf.reshape(weights, (-1, weights.shape[-1], 1))\n",
    "            all_predictions.append(tf.concat([weights_reshaped, means, stds], axis=1))\n",
    "\n",
    "        # predictions.shape => (time, batch, features)\n",
    "        # print('concat', all_predictions[0].shape)\n",
    "        predictions = tf.concat(all_predictions, axis=-1)\n",
    "        # print('stack', predictions.shape)\n",
    "        # predictions.shape => (batch, time, features)\n",
    "        predictions = tf.transpose(predictions, [0, 2, 1])\n",
    "        # print('transpose', predictions.shape)\n",
    "        return predictions\n",
    "    \n",
    "    def load_weights_and_build(self, weights_path):\n",
    "        self.build(input_shape=(None, self.timesteps, self.num_features))\n",
    "        self.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71a7b992-4c7d-404a-9337-97018c170a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    y_true = tf.reduce_max(y_true, axis=1, keepdims=True)\n",
    "    \n",
    "    tn = tf.keras.backend.sum(tf.keras.backend.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tp = tf.keras.backend.sum(tf.keras.backend.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fn = tf.keras.backend.sum(tf.keras.backend.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fp = tf.keras.backend.sum(tf.keras.backend.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "    r = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+tf.keras.backend.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - tf.keras.backend.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cadf149d-dcce-4cb3-821b-ff6c75d93af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_loss(y_true, y_pred):\n",
    "    y_true = tf.reduce_max(y_true, axis=1, keepdims=True)\n",
    "    loss = tf.abs(y_true - y_pred)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30bd2123-2fa6-4656-b231-34b8a8ded519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_custom_accuracy_loss(num_mixtures, output_features):\n",
    "    def accuracy_loss(y_true, y_pred):\n",
    "        def sample_mean(weights, means):\n",
    "            \"\"\"Sample from the mixture of Gaussians\"\"\"\n",
    "            component_idx = tf.argmax(weights, axis=-1)\n",
    "            selected_means = tf.gather(means, component_idx, batch_dims=2)  # shape (batch, output_dim)\n",
    "            return selected_means\n",
    "        \n",
    "        window =  sample_mean(\n",
    "                y_pred[:, :, :num_mixtures],\n",
    "                y_pred[:, :, num_mixtures:num_mixtures + num_mixtures*output_features]\n",
    "            ) # l'accesso con iteratore serve perché sample_mixture ritorna un array con shape (batch, output_features)\n",
    "        \n",
    "        window = tf.reshape(window, (-1, window.shape[1], 1))\n",
    "        window = tf.reduce_max(window, axis=1, keepdims=True)\n",
    "        y_true = tf.reduce_max(y_true, axis=1, keepdims=True)\n",
    "        loss = tf.abs(y_true - window)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return accuracy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78e8d09e-1fa4-430d-846d-a7a5b22f4489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_custom_loss(num_mixtures, output_features):\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        def sample_mean(weights, means):\n",
    "            \"\"\"Sample from the mixture of Gaussians\"\"\"\n",
    "            component_idx = tf.argmax(weights, axis=-1)            \n",
    "            selected_means = tf.gather(means, component_idx, batch_dims=2)  # shape (batch, output_dim)\n",
    "            \n",
    "            return selected_means\n",
    "        \n",
    "        window =  sample_mean(\n",
    "                y_pred[:, :, :num_mixtures],\n",
    "                y_pred[:, :, num_mixtures:num_mixtures + num_mixtures*output_features]\n",
    "            ) # l'accesso con iteratore serve perché sample_mixture ritorna un array con shape (batch, output_features)\n",
    "        \n",
    "        window = tf.reshape(window, (-1, window.shape[1], 1))\n",
    "        absolute_error = tf.abs(y_true - window)\n",
    "        loss = tf.reduce_sum(absolute_error, axis=1) / tf.cast(tf.shape(y_true)[1], tf.float32)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ea1e29-9a3e-4099-aaaa-48a51ac41e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_combined_custom_loss(num_mixtures, output_features):\n",
    "    def combined_custom_loss(y_true, y_pred):\n",
    "        def sample_mean(weights, means):\n",
    "            \"\"\"Sample from the mixture of Gaussians\"\"\"\n",
    "            component_idx = tf.argmax(weights, axis=-1)            \n",
    "            # Gather the means and stds based on the sampled component index\n",
    "            selected_means = tf.gather(means, component_idx, batch_dims=2)  # shape (batch, output_dim)\n",
    "            \n",
    "            return selected_means\n",
    "        \n",
    "        window =  sample_mean(\n",
    "                y_pred[:, :, :num_mixtures],\n",
    "                y_pred[:, :, num_mixtures:num_mixtures + num_mixtures*output_features]\n",
    "            ) # l'accesso con iteratore serve perché sample_mixture ritorna un array con shape (batch, output_features)\n",
    "        \n",
    "        window = tf.reshape(window, (-1, window.shape[1], 1))\n",
    "        absolute_error = tf.abs(y_true - window)\n",
    "        mae = tf.reduce_sum(absolute_error, axis=1) / tf.cast(tf.shape(y_true)[1], tf.float32)\n",
    "\n",
    "        window = tf.reduce_max(window, axis=1, keepdims=True)\n",
    "        y_true = tf.reduce_max(y_true, axis=1, keepdims=True)\n",
    "        acc = tf.abs(y_true - window)\n",
    "\n",
    "        return mae - (1 - acc) * 0.2\n",
    "    \n",
    "    return combined_custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc2af5d6-d2ef-4dca-bd6e-3e6d221cd9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bce_loss(num_mixtures, output_features):\n",
    "    def bce_loss(y_true, y_pred):\n",
    "        def sample_mean(weights, means):\n",
    "            \"\"\"Sample from the mixture of Gaussians\"\"\"\n",
    "            component_idx = tf.argmax(weights, axis=-1)            \n",
    "            # Gather the means and stds based on the sampled component index\n",
    "            selected_means = tf.gather(means, component_idx, batch_dims=2)  # shape (batch, output_dim)\n",
    "            \n",
    "            return selected_means\n",
    "        \n",
    "        window =  sample_mean(\n",
    "                y_pred[:, :, :num_mixtures],\n",
    "                y_pred[:, :, num_mixtures:num_mixtures + num_mixtures*output_features]\n",
    "            ) # l'accesso con iteratore serve perché sample_mixture ritorna un array con shape (batch, output_features)\n",
    "\n",
    "        loss = tf.keras.losses.Hinge()(y_true, window)\n",
    "        return loss\n",
    "    \n",
    "    return bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32cbdfd6-330e-4081-8c5c-c4b9c37241c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mdn_loss(num_mixtures, output_dim):\n",
    "    def mdn_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the MDN loss using the log-likelihood of the true values \n",
    "        under the predicted mixture of Gaussians.\n",
    "\n",
    "        y_true: True target values (batch_size, time_steps, output_dim)\n",
    "        y_pred: Predicted values from the model (batch_size, time_steps, num_mixtures * output_dim * 2 + num_mixtures)\n",
    "        num_mixtures: Number of mixtures (K)\n",
    "        output_dim: Dimension of the output (D)\n",
    "        \"\"\"\n",
    "        # Extract mixture weights, means, and stds from the predicted tensor\n",
    "        weights = y_pred[:, :, :num_mixtures]\n",
    "        means = y_pred[:, :, num_mixtures:num_mixtures + num_mixtures * output_dim]\n",
    "        stds = y_pred[:, :, num_mixtures + num_mixtures * output_dim:]\n",
    "        \n",
    "        # Reshape means and stds to match (batch_size, time_steps, num_mixtures, output_dim)\n",
    "        means = tf.reshape(means, (-1, tf.shape(means)[1], num_mixtures, output_dim))\n",
    "        stds = tf.reshape(stds, (-1, tf.shape(stds)[1], num_mixtures, output_dim))\n",
    "\n",
    "        # Compute the probability density for each Gaussian component\n",
    "        # For each mixture, calculate the probability of the true value given the mean and std\n",
    "        # norm_dist = tfd.Normal(loc=means, scale=stds)\n",
    "        norm_dist = tf.compat.v1.distributions.Normal(loc=means, scale=stds)\n",
    "        log_prob = norm_dist.log_prob(tf.expand_dims(y_true, axis=-2))  # (batch_size, time_steps, num_mixtures, output_dim)\n",
    "\n",
    "        # Sum over the output dimension (to integrate over all the Gaussians in the mixture)\n",
    "        log_prob_sum = tf.reduce_sum(log_prob, axis=-1)  # (batch_size, time_steps, num_mixtures)\n",
    "\n",
    "        # Apply the mixture weights and compute the weighted log-likelihood\n",
    "        weighted_log_prob = log_prob_sum + tf.math.log(weights)  # (batch_size, time_steps, num_mixtures)\n",
    "\n",
    "        # Sum over all mixtures to get the total log-likelihood for each sample\n",
    "        total_log_prob = tf.reduce_logsumexp(weighted_log_prob, axis=-1)  # (batch_size, time_steps)\n",
    "\n",
    "        # Return the negative log-likelihood (since we minimize loss)\n",
    "        return -tf.reduce_mean(total_log_prob)\n",
    "    \n",
    "    return mdn_loss\n",
    "def get_mixture_loss_func(output_dim, num_mixes):\n",
    "    def mdn_loss_func(y_true, y_pred):\n",
    "        # Reshape inputs in case this is used in a TimeDistributed layer\n",
    "        y_pred = tf.reshape(\n",
    "            y_pred,\n",
    "            [-1, (2 * num_mixes * output_dim) + num_mixes],\n",
    "            name=\"reshape_ypreds\",\n",
    "        )\n",
    "        y_true = tf.reshape(y_true, [-1, output_dim], name=\"reshape_ytrue\")\n",
    "        # Split the inputs into parameters\n",
    "        out_mu, out_sigma, out_pi = tf.split(\n",
    "            y_pred,\n",
    "            num_or_size_splits=[\n",
    "                num_mixes * output_dim,\n",
    "                num_mixes * output_dim,\n",
    "                num_mixes,\n",
    "            ],\n",
    "            axis=-1,\n",
    "            name=\"mdn_coef_split\",\n",
    "        )\n",
    "        # Construct the mixture models\n",
    "        cat = tf.compat.v1.Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mixes\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [\n",
    "            tf.compat.v1.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n",
    "            for loc, scale in zip(mus, sigs)\n",
    "        ]\n",
    "        mixture = tf.compat.v1.Mixture(cat=cat, components=coll)\n",
    "        loss = mixture.log_prob(y_true)\n",
    "        loss = tf.negative(loss)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "\n",
    "    return mdn_loss_func\n",
    "# Custom callbacks\n",
    "class CustomModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1, \n",
    "                 initial_train_loss=float('inf'), initial_val_loss=float('inf'), save_weights_only=True, min_loss_delta=None):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.save_best_only = save_best_only\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        # Initializing best loss values with the provided thresholds\n",
    "        self.best_train_loss = initial_train_loss\n",
    "        self.best_val_loss = initial_val_loss\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.min_loss_delta = min_loss_delta\n",
    "        self.last_improvement_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        train_loss = logs.get('loss')\n",
    "        val_loss = logs.get(self.monitor)\n",
    "\n",
    "        if train_loss is None or val_loss is None:\n",
    "            return\n",
    "\n",
    "        # Check if both training and validation loss have improved\n",
    "        if self.mode == 'min':\n",
    "            if ((self.min_loss_delta is None and val_loss < self.best_val_loss) \n",
    "                or (self.min_loss_delta is not None and val_loss < (self.best_val_loss - self.min_loss_delta))):\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"\\nEpoch {epoch+1}: Val loss improved, from {self.best_val_loss} to {val_loss}\")\n",
    "                self.best_train_loss = train_loss\n",
    "                self.best_val_loss = val_loss\n",
    "                if self.save_weights_only:\n",
    "                    self.model.save_weights(self.filepath)\n",
    "                else:\n",
    "                    self.model.save(self.filepath)\n",
    "            elif train_loss < self.best_train_loss and self.best_val_loss < val_loss:\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"\\nEpoch {epoch+1}: Train loss improved, from {self.best_train_loss} to {train_loss}\")\n",
    "                self.best_train_loss = train_loss\n",
    "                self.best_val_loss = val_loss\n",
    "                if self.save_weights_only:\n",
    "                    self.model.save_weights(self.filepath)\n",
    "                else:\n",
    "                    self.model.save(self.filepath)\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"\\nEpoch {epoch+1}: Train and val losses didn't improve. Best val {self.best_val_loss}, Best Train {self.best_train_loss}\")\n",
    "                    self.model.load_weights(self.filepath)\n",
    "        elif self.mode == 'max':\n",
    "            raise ValueError(\"Mode 'max' is not supported for this callback. Only 'min' mode is supported.\")\n",
    "class RestoreBestWeightsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, checkpoint_filepath):\n",
    "        self.checkpoint_filepath = checkpoint_filepath\n",
    "        self.best_val_loss = math.inf\n",
    "        self.last_improved_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get(\"val_loss\")\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.last_improved_epoch = epoch + 1\n",
    "            self.best_val_loss = val_loss\n",
    "\n",
    "        # Load the best weights at the end of each epoch\n",
    "        if epoch > 0:  # Avoid loading weights at the very first epoch\n",
    "            self.model.load_weights(self.checkpoint_filepath)\n",
    "            print(f\"Epoch {epoch+1}: Restoring model weights from {self.checkpoint_filepath} of epoch {self.last_improved_epoch} with val loss {self.best_val_loss}, after epoch {epoch+1}.\")\n",
    "class AdaptiveLRScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, patience=5, increase_factor=1.5, decrease_factor=0.5, min_lr=1e-6, min_delta=0.1):\n",
    "        super().__init__()\n",
    "        self.patience = patience  # Number of epochs with no improvement before decreasing LR\n",
    "        self.increase_factor = increase_factor  # Factor by which to increase LR\n",
    "        self.decrease_factor = decrease_factor  # Factor by which to decrease LR\n",
    "        self.min_lr = min_lr  # Minimum value to which LR can decrease\n",
    "        self.best_loss = np.inf\n",
    "        self.epochs_since_improvement = 0\n",
    "        self.epochs_with_slow_improvement = 0\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get(\"val_loss\")\n",
    "        \n",
    "        # If there's improvement, reset the \"no improvement\" counter and save the best loss\n",
    "        if current_loss < self.best_loss:\n",
    "            if self.best_loss - current_loss <= self.min_delta:\n",
    "                self.epochs_with_slow_improvement += 1\n",
    "            else:\n",
    "                self.epochs_with_slow_improvement = 0\n",
    "\n",
    "            self.best_loss = current_loss\n",
    "            self.epochs_since_improvement = 0\n",
    "        else:\n",
    "            self.epochs_since_improvement += 1\n",
    "            self.epochs_with_slow_improvement = 0\n",
    "        \n",
    "        # Apply learning rate adjustment\n",
    "        current_lr = self.model.optimizer.lr.numpy()\n",
    "\n",
    "        if self.epochs_since_improvement >= self.patience:\n",
    "            # No improvement for `patience` epochs, reduce learning rate\n",
    "            new_lr = max(current_lr * self.decrease_factor, self.min_lr)\n",
    "            self.model.optimizer.lr.assign(new_lr)\n",
    "            print(f\"Epoch {epoch+1}: Reducing learning rate to {new_lr:.6f}\")\n",
    "            self.epochs_since_improvement = 0  # Reset after decreasing LR\n",
    "        elif self.epochs_with_slow_improvement >= self.patience:\n",
    "            # If there has been improvement, we may want to increase the learning rate\n",
    "            # We only increase the LR if the rate of progress is slow (i.e., if no major improvement over the last few epochs)\n",
    "            new_lr = current_lr * self.increase_factor\n",
    "            self.model.optimizer.lr.assign(new_lr)\n",
    "            self.epochs_with_slow_improvement = 0\n",
    "            print(f\"Epoch {epoch+1}: Increasing learning rate to {new_lr:.6f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}: No variation to the learning rate {current_lr:.6f}; incresing countdown {(self.patience-self.epochs_with_slow_improvement)} - decreasing countdown {self.patience-self.epochs_since_improvement}\")\n",
    "# Compile & Fit\n",
    "MAX_EPOCHS = 150\n",
    "\n",
    "def compile_and_fit(\n",
    "    model,\n",
    "    validation_set,\n",
    "    x_training_set=None,\n",
    "    y_training_set=None,\n",
    "    batch_size=1,\n",
    "    training_data_generator=None,\n",
    "    shuffle=False,\n",
    "    checkpoint_threshold=math.inf,\n",
    "    best_weights_path=\"Dati/USE_CASE/pChurn/best_models/model.weights.h5\",\n",
    "    initial_lr=1e-3\n",
    "):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        min_delta=0.02,\n",
    "        mode=\"min\",\n",
    "        restore_best_weights=False,\n",
    "    )\n",
    "\n",
    "    # Usage Example:\n",
    "    # Initialize the custom callback with initial loss thresholds\n",
    "    custom_checkpoint_callback = CustomModelCheckpoint(\n",
    "        filepath=base_path+best_weights_path,\n",
    "        verbose=1,\n",
    "        min_loss_delta=0.3\n",
    "        # initial_train_loss=1.5,  # Example initial training loss threshold\n",
    "        # initial_val_loss=1.5     # Example initial validation loss threshold\n",
    "    )\n",
    "\n",
    "    reduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "        verbose=0,\n",
    "        mode=\"min\",\n",
    "        min_delta=0.1,\n",
    "        cooldown=0,\n",
    "        min_lr=0.0,\n",
    "    )\n",
    "\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=base_path+best_weights_path,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        initial_value_threshold=checkpoint_threshold\n",
    "    )\n",
    "\n",
    "    restore_best_weights = RestoreBestWeightsCallback(base_path+best_weights_path)\n",
    "\n",
    "    adaptive_lr = AdaptiveLRScheduler(patience=3, \n",
    "                                      increase_factor=10, \n",
    "                                      decrease_factor=0.1, \n",
    "                                      min_lr=1e-10, \n",
    "                                      min_delta=0.001)\n",
    "\n",
    "    model.compile(\n",
    "        # loss=tf.losses.BinaryCrossentropy(),\n",
    "        loss=f1_loss,\n",
    "        # loss=tf.losses.Hinge(),\n",
    "        # loss=get_mixture_loss_func(1, model.num_mixtures),\n",
    "        # loss=get_mdn_loss(model.num_mixtures, model.num_output_features),\n",
    "        # loss=get_bce_loss(model.num_mixtures, model.num_output_features),\n",
    "        # loss=get_custom_loss(model.num_mixtures, model.num_output_features),\n",
    "        # loss=get_custom_accuracy_loss(model.num_mixtures, model.num_output_features),\n",
    "        # loss=accuracy_loss,\n",
    "        # optimizer=tf.keras.mixed_precision.LossScaleOptimizer(tf.optimizers.Adam(learning_rate=initial_lr, clipvalue=10.0)),\n",
    "        # optimizer=tf.keras.optimizers.RMSprop(learning_rate=initial_lr),\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate=initial_lr),\n",
    "        # optimizer=tf.keras.optimizers.SGD(learning_rate=initial_lr, momentum=0.9),\n",
    "        # metrics=[tf.keras.losses.MeanAbsoluteError()],\n",
    "        metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.Precision()],\n",
    "    )\n",
    "\n",
    "    if training_data_generator is not None:\n",
    "        history = model.fit(\n",
    "            training_data_generator,\n",
    "            epochs=MAX_EPOCHS,\n",
    "            validation_data=validation_set,\n",
    "            shuffle=shuffle,\n",
    "            callbacks=[model_checkpoint, reduce_on_plateau, early_stopping, restore_best_weights],\n",
    "        )\n",
    "    else:\n",
    "        history = model.fit(\n",
    "            x=x_training_set,\n",
    "            y=y_training_set,\n",
    "            epochs=MAX_EPOCHS,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=validation_set,\n",
    "            shuffle=shuffle,\n",
    "            callbacks=[model_checkpoint, adaptive_lr, early_stopping, restore_best_weights],\n",
    "        )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720aecdb-4790-4327-9c44-38b8545048cb",
   "metadata": {},
   "source": [
    "# Caricamento Dataset con Numpy partizionati da Pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703a707b-3c5e-42eb-83d4-e6a902b460f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 09:00:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "java_options = (\n",
    "    \"--add-opens=java.base/java.lang=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.nio=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.net=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.util=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.util.calendar=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.lang.invoke=ALL-UNNAMED\"\n",
    ")\n",
    "\n",
    "def inizialization(notebook, \n",
    "                   instances, \n",
    "                   cores, \n",
    "                   ram,\n",
    "                   min_instances=1):\n",
    "    machine_set_up = {\n",
    "        \"svil1\": { \"appName\": \"spark-svil1\", \"spark.driver.port\": \"8002\", \"spark.blockManager.port\": \"8001\", },\n",
    "        \"svil2\": { \"appName\": \"spark-svil2\", \"spark.driver.port\": \"18002\", \"spark.blockManager.port\": \"18001\", },\n",
    "        \"svil3\": { \"appName\": \"spark-svil3\", \"spark.driver.port\": \"28002\", \"spark.blockManager.port\": \"28001\", },\n",
    "    }\n",
    "\n",
    "    pod_ip = os.environ.get(\"POD_IP\", \"0.0.0.0\")\n",
    "\n",
    "    return (\n",
    "        SparkSession.builder.master(\"k8s://https://kubernetes.default.svc:443\")\n",
    "        .appName(machine_set_up[notebook][\"appName\"])\n",
    "        .config(\"spark.submit.deployMode\", \"client\")\n",
    "\n",
    "        # --- Configurazione Esecutori Kubernetes ---\n",
    "        .config(\"spark.kubernetes.namespace\", \"spark-svil\")\n",
    "        .config(\"spark.kubernetes.container.image\", \"europe-west6-docker.pkg.dev/poc-example-ds/pyspark-images/spark-py-worker:latest\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark-sa-svil\")\n",
    "        .config(\"spark.kubernetes.authenticate.executor.serviceAccountName\", \"spark-sa-svil\")\n",
    "        .config(\"spark.executor.memory\", f\"{ram}g\")\n",
    "        .config(\"spark.executor.cores\", cores)\n",
    "        .config(\"spark.kubernetes.executor.nodeSelector.workload\", \"ssd-shuffle\") \n",
    "        .config(\"spark.kubernetes.executor.memoryOverhead\", \"6g\")\n",
    "\n",
    "\n",
    "        # --- Configurazione Driver ---\n",
    "        .config(\"spark.driver.host\", pod_ip)\n",
    "        .config(\"spark.driver.port\", machine_set_up[notebook][\"spark.driver.port\"])\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "        .config(\"spark.driver.memory\", \"8g\")\n",
    "        .config(\"spark.files.maxPartitionBytes\",\"16777216\")\n",
    "        #.config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\n",
    "        .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.jupyter-workspace.mount.path\", \"/home/jovyan/work\")\n",
    "        .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.jupyter-workspace.options.claimName\", \"jupyter-workspace-storage\")\n",
    "        # --- Altre configurazioni ---\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\", min_instances)\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", instances)\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .config(\"spark.driver.local.dir\", \"/tmp/spark-driver\") \n",
    "\n",
    "       # --- CONFIGURAZIONE SSD PER EXECUTOR ---\n",
    "        # Dice agli executor di usare l'SSD per lo shuffle\n",
    "        .config(\"spark.executor.extraClassPath\", \"/mnt/disks/ssd0\") # Legacy, ma aiuta\n",
    "        .config(\"spark.kubernetes.executor.local.dirs.tmpfs\", \"/mnt/disks/ssd0\") # Metodo Kubernetes\n",
    "        # L'init container per impostare i permessi sull'SSD degli executor\n",
    "        .config(\"spark.kubernetes.executor.initContainer.image\", \"busybox:1.28\")\n",
    "        .config(\"spark.kubernetes.executor.initContainer.command\", '[\"/bin/sh\", \"-c\", \"chmod 777 /mnt/disks/ssd0\"]')\n",
    "        #.config(\"spark.memory.fraction\", 0.8) # Usa l'80% della memoria per Spark\n",
    "        #.config(\"spark.memory.storageFraction\", 0.3) # Di cui, solo il 30% per il cache\n",
    "        .config(\"spark.sql.shuffle.partitions\", 200)\n",
    "        .config(\"spark.shuffle.file.buffer\", \"1m\") # Aumenta il buffer a 1MB (default 32k)\n",
    "        # Nel tuo builder della SparkSession\n",
    "        #.config(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")  # Assicura che sia attivo\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") # Ottimizzazione aggiuntiva\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") # Gestisce lo skew specificamente per i join\n",
    "       .getOrCreate()\n",
    "    )\n",
    "spark = inizialization(\n",
    "    \"svil3\",\n",
    "    instances=28,\n",
    "    cores=7,\n",
    "    ram=48,\n",
    "    min_instances=28\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c40a33-8829-40f0-9f70-66bbc942bd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 09:00:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:00:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:00:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:01:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:01:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:01:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:01:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:02:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:02:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:02:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:02:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:03:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:03:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:03:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:03:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(base_path)\n\u001b[1;32m      5\u001b[0m df_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_series\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDati/USE_CASE/pChurn/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdf_prefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_df_train.parquet/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m df_val \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDati/USE_CASE/pChurn/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_df_val.parquet/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m df_train \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mDati/USE_CASE/pChurn/numpy_partitions/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "base_path = \"gs://poc-example-dati-dev/\" #\"/opt/spark/work-dir/\" -> Uppato a Google Cloud Storage\n",
    "sys.path.append(base_path)\n",
    "\n",
    "df_prefix = \"time_series\"\n",
    "df_train = spark.read.parquet(base_path + f\"Dati/USE_CASE/pChurn/{df_prefix}_df_train.parquet/\")\n",
    "df_val = spark.read.parquet(base_path + f\"Dati/USE_CASE/pChurn/{df_prefix}_df_val.parquet/\")\n",
    "\n",
    "df_train = df_train.withColumn(\"path\", F.lit(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/train\"))\n",
    "df_train = df_train.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "df_train = df_train.withColumn(\"partition_idx\", F.spark_partition_id())\n",
    "df_val = df_val.withColumn(\"path\", F.lit(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/val\"))\n",
    "df_val = df_val.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "df_val = df_val.withColumn(\"partition_idx\", F.spark_partition_id())\n",
    "\n",
    "if os.path.exists(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/train/\"):\n",
    "    shutil.rmtree(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/train/\")\n",
    "if os.path.exists(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/val/\"):\n",
    "    shutil.rmtree(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/val/\")\n",
    "\n",
    "os.makedirs(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/train\", exist_ok=True)\n",
    "os.makedirs(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/train/sample\", exist_ok=True)\n",
    "os.makedirs(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/train/target\", exist_ok=True)\n",
    "os.makedirs(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/val\", exist_ok=True)\n",
    "os.makedirs(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/val/sample\", exist_ok=True)\n",
    "os.makedirs(f\"{base_path}Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/val/target\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8efaa26-0b55-485f-a800-b8b70cec290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. FUNZIONE PER CONVERTIRE LE PARTIZIONI SPARK IN TENSORI ---\n",
    "# Questa funzione verrà eseguita in parallelo su ogni worker Spark.\n",
    "# Il suo scopo è trasformare le righe Spark in tensori NumPy.\n",
    "def spark_partition_to_tensors(partition_iterator):\n",
    "    # Converte l'iteratore della partizione in un DataFrame pandas (efficiente a livello di partizione)\n",
    "    pdf = pd.DataFrame([row.asDict() for row in partition_iterator])\n",
    "    \n",
    "    # Se la partizione è vuota, esci.\n",
    "    if pdf.empty:\n",
    "        return\n",
    "\n",
    "    # Estrai le colonne 'sample' e 'target' e convertile in array NumPy pronti per TF\n",
    "    # np.stack converte una lista di array in un unico array multidimensionale.\n",
    "    samples = np.stack(pdf[\"sample\"].apply(lambda x: x.toArray()).values)\n",
    "    targets = np.stack(pdf[\"target\"].values)\n",
    "    \n",
    "    # \"Yield\" crea un generatore, che è efficiente in termini di memoria.\n",
    "    # Restituisce una singola tupla (samples, targets) per l'intera partizione.\n",
    "    yield samples, targets\n",
    "\n",
    "# --- 2. CREA I TF.DATA.DATASET USANDO UN GENERATORE (Metodo Corretto) ---\n",
    "\n",
    "# Ottieni le specifiche dei dati per aiutare TensorFlow a capire la struttura\n",
    "# Il primo 'None' rappresenta la dimensione del batch, che può variare.\n",
    "sample_spec = tf.TensorSpec(shape=(None, 4, 22), dtype=tf.float32)\n",
    "target_spec = tf.TensorSpec(shape=(None, 2), dtype=tf.float32)\n",
    "output_signature = (sample_spec, target_spec)\n",
    "\n",
    "# Crea un \"generatore\" che tira i dati dai worker una partizione alla volta\n",
    "# .toLocalIterator() è memory-efficient: non carica tutto il dataset sul driver.\n",
    "train_generator = lambda: df_train.rdd.mapPartitions(spark_partition_to_tensors).toLocalIterator()\n",
    "\n",
    "# Crea la pipeline di dati di TensorFlow usando il generatore\n",
    "# TensorFlow ora chiederà a Spark le partizioni una per una, man mano che servono.\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    train_generator,\n",
    "    output_signature=output_signature\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Fai lo stesso per il validation set\n",
    "val_generator = lambda: df_val.rdd.mapPartitions(spark_partition_to_tensors).toLocalIterator()\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    val_generator,\n",
    "    output_signature=output_signature\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Pipeline di dati TensorFlow create correttamente in memoria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734dc1d1-16ac-435f-ab92-80926247573f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 09:04:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:04:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:04:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:04:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:05:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:05:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:05:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:05:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:06:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:06:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:06:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:06:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:07:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:07:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:07:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:07:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:08:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:08:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:08:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:08:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:09:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:09:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:09:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:09:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:10:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:10:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:10:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:10:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:11:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:11:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:11:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:11:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:12:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:12:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:12:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:12:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:13:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:13:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:13:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:13:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:14:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:14:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:14:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:14:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:15:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:15:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:15:42 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:15:57 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:16:12 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:16:27 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/07/28 09:16:37 ERROR AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
      "java.lang.NullPointerException: null\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.getGcsFs(GoogleHadoopFileSystem.java:1679) ~[gcs-connector-3.0.7.jar:?]\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.commitTempFile(GoogleHadoopOutputStream.java:379) ~[gcs-connector-3.0.7.jar:?]\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.hsyncInternal(GoogleHadoopOutputStream.java:333) ~[gcs-connector-3.0.7.jar:?]\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.lambda$hflush$2(GoogleHadoopOutputStream.java:294) ~[gcs-connector-3.0.7.jar:?]\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GhfsGlobalStorageStatistics.trackDuration(GhfsGlobalStorageStatistics.java:117) ~[gcs-connector-3.0.7.jar:?]\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.trackDurationWithTracing(GoogleHadoopOutputStream.java:249) ~[gcs-connector-3.0.7.jar:?]\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.hflush(GoogleHadoopOutputStream.java:280) ~[gcs-connector-3.0.7.jar:?]\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:136) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.spark.deploy.history.EventLogFileWriter.$anonfun$writeLine$3(EventLogFileWriters.scala:122) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.deploy.history.EventLogFileWriter.$anonfun$writeLine$3$adapted(EventLogFileWriters.scala:122) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]\n",
      "\tat org.apache.spark.deploy.history.EventLogFileWriter.writeLine(EventLogFileWriters.scala:122) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.writeEvent(EventLogFileWriters.scala:229) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:97) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.onApplicationEnd(EventLoggingListener.scala:181) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23) ~[scala-library-2.12.18.jar:?]\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62) ~[scala-library-2.12.18.jar:?]\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1358) [spark-core_2.12-3.5.3.jar:3.5.3]\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96) [spark-core_2.12-3.5.3.jar:3.5.3]\n"
     ]
    }
   ],
   "source": [
    "# Otteniamo il conteggio totale dei campioni direttamente dai DataFrame Spark.\n",
    "# Questa è un'operazione veloce che non causa problemi di memoria.\n",
    "train_count = df_train.count()\n",
    "val_count = df_val.count()\n",
    "\n",
    "print(f\"Numero totale di campioni di training: {train_count}\")\n",
    "print(f\"Numero totale di campioni di validazione: {val_count}\")\n",
    "\n",
    "# La forma di un singolo campione (es. (4, 22)) è definita dalla logica di preprocessing\n",
    "print(f\"Forma di un singolo campione (sample): (4, 22)\")\n",
    "print(f\"Forma di un singolo target: (2, 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac800a7e-a090-4964-9039-c55795b40bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b443ca5-867b-49e7-a504-f3f551dc7d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration (Adjust these paths and prefixes) ---\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np # Assuming you'd convert to numpy arrays if reading directly\n",
    "import sys\n",
    "base_path = \"gs://poc-example-dati-dev/\" #\"/opt/spark/work-dir/\" -> Uppato a Google Cloud Storage\n",
    "sys.path.append(base_path)\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import time\n",
    "\n",
    "print(\"Timer avviato...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_prefix = \"time_series\"\n",
    "\n",
    "# --- Define paths for NumPy partitions (where samples/targets will be saved/loaded) ---\n",
    "# These paths are for saving/loading individual NumPy files, mimicking the Spark output structure.\n",
    "train_numpy_path = os.path.join(base_path, f\"Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/train\")\n",
    "val_numpy_path = os.path.join(base_path, f\"Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/val\")\n",
    "\n",
    "train_sample_dir = os.path.join(train_numpy_path, \"sample\")\n",
    "train_target_dir = os.path.join(train_numpy_path, \"target\")\n",
    "val_sample_dir = os.path.join(val_numpy_path, \"sample\")\n",
    "val_target_dir = os.path.join(val_numpy_path, \"target\")\n",
    "\n",
    "# --- 1. Directory Setup (Standard Python - remains similar) ---\n",
    "print(\"Setting up directories...\")\n",
    "if os.path.exists(train_numpy_path):\n",
    "    shutil.rmtree(train_numpy_path)\n",
    "if os.path.exists(val_numpy_path):\n",
    "    shutil.rmtree(val_numpy_path)\n",
    "\n",
    "os.makedirs(train_sample_dir, exist_ok=True)\n",
    "os.makedirs(train_target_dir, exist_ok=True)\n",
    "os.makedirs(val_sample_dir, exist_ok=True)\n",
    "os.makedirs(val_target_dir, exist_ok=True)\n",
    "print(\"Directories created.\")\n",
    "\n",
    "# --- 2. Data Simulation (REPLACE WITH ACTUAL TFRECORD LOADING) ---\n",
    "# IMPORTANT: This section simulates having TFRecord files.\n",
    "# In a real scenario, you would have converted your Spark Parquet DataFrames\n",
    "# into TFRecord files in a prior, distributed step (e.g., using Spark, Dataflow, or a custom script).\n",
    "# Each TFRecord file would contain serialized 'sample' and 'target' features.\n",
    "\n",
    "# Define the expected shape and dtype of your sample and target features\n",
    "# (4, 22) for sample, (2,) for target (assuming target is a 1D array of 2 elements)\n",
    "SAMPLE_SHAPE = (4, 22)\n",
    "TARGET_SHAPE = (2,)\n",
    "DTYPE = tf.float32\n",
    "\n",
    "# Simulate creating some dummy TFRecord files\n",
    "# In production, these would be generated from your actual data\n",
    "def _serialize_example(sample_feature, target_feature):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        'sample': tf.train.Feature(float_list=tf.train.FloatList(value=sample_feature.flatten())),\n",
    "        'target': tf.train.Feature(float_list=tf.train.FloatList(value=target_feature.flatten()))\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "\n",
    "def create_dummy_tfrecords(num_records, filename):\n",
    "    writer = tf.io.TFRecordWriter(filename)\n",
    "    for i in range(num_records):\n",
    "        # Simulate different 'target' values for balanced sampling\n",
    "        target_val = 1.0 if i % 2 == 0 else 0.0 # Roughly 50/50 active/inactive\n",
    "        sample = np.random.rand(*SAMPLE_SHAPE).astype(np.float32)\n",
    "        target = np.array([target_val, target_val]).astype(np.float32) # Assuming target is 2 elements\n",
    "        writer.write(_serialize_example(sample, target))\n",
    "    writer.close()\n",
    "    print(f\"Created {num_records} dummy TFRecords in {filename}\")\n",
    "\n",
    "# Create dummy TFRecord files for demonstration\n",
    "dummy_train_tfrecord_path = os.path.join(train_numpy_path, \"dummy_train.tfrecord\")\n",
    "dummy_val_tfrecord_path = os.path.join(train_numpy_path, \"dummy_val.tfrecord\")\n",
    "create_dummy_tfrecords(10000, dummy_train_tfrecord_path) # Simulate 10,000 records\n",
    "create_dummy_tfrecords(2000, dummy_val_tfrecord_path)   # Simulate 2,000 records\n",
    "\n",
    "\n",
    "# --- 3. TensorFlow Data Pipeline for Loading and Transformations ---\n",
    "print(\"Building TensorFlow data pipelines...\")\n",
    "\n",
    "def _parse_tfrecord_fn(example_proto):\n",
    "    \"\"\"\n",
    "    Parses a single tf.train.Example proto and returns features and labels.\n",
    "    \"\"\"\n",
    "    feature_description = {\n",
    "        'sample': tf.io.FixedLenFeature(SAMPLE_SHAPE[0] * SAMPLE_SHAPE[1], tf.float32),\n",
    "        'target': tf.io.FixedLenFeature(TARGET_SHAPE[0], tf.float32) # Adjusted for target shape\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Reshape 'sample' and 'target' to their original dimensions\n",
    "    sample = tf.reshape(example['sample'], SAMPLE_SHAPE)\n",
    "    target = tf.reshape(example['target'], TARGET_SHAPE)\n",
    "    \n",
    "    return sample, target\n",
    "\n",
    "def create_tf_dataset(tfrecord_path, is_training=True):\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset from TFRecord files.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    dataset = dataset.map(_parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Simulate Spark's .withColumn(\"path\", ...) and other metadata\n",
    "    # In tf.data, you usually add metadata as part of the feature set or manage it externally.\n",
    "    # For 'row_idx' and 'partition_idx', these are often implicit in tf.data.Dataset\n",
    "    # if you're not explicitly saving them as features.\n",
    "    # If you need them, you'd add them as features in your TFRecord.\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1024) # Shuffle for training\n",
    "    \n",
    "    dataset = dataset.batch(32) # Example batch size\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_tf_dataset(dummy_train_tfrecord_path, is_training=True)\n",
    "val_dataset = create_tf_dataset(dummy_val_tfrecord_path, is_training=False)\n",
    "\n",
    "print(\"TensorFlow data pipelines created correctly.\")\n",
    "\n",
    "# --- 4. Balanced Sampling (Conceptual with tf.data.Dataset) ---\n",
    "# This is a complex operation for very large, imbalanced datasets in TF.\n",
    "# For exact balancing like Spark's sample, you might need to filter and combine.\n",
    "print(\"\\nStarting conceptual balanced sampling with TensorFlow (in memory)...\")\n",
    "\n",
    "def balanced_sample_tf_dataset(dataset, fraction_to_sample=0.05):\n",
    "    \"\"\"\n",
    "    Conceptually samples a TensorFlow dataset to achieve better class balance.\n",
    "    This assumes 'target' contains a class indicator (e.g., 0 or 1).\n",
    "    For very large datasets, this might be memory intensive if classes are\n",
    "    extracted to separate datasets in memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate active and inactive samples\n",
    "    active_dataset = dataset.filter(lambda s, t: tf.reduce_max(t) >= 1)\n",
    "    inactive_dataset = dataset.filter(lambda s, t: tf.reduce_max(t) < 1)\n",
    "\n",
    "    # Get counts (requires iterating the dataset, can be slow for large datasets)\n",
    "    active_count = tf.data.experimental.cardinality(active_dataset).numpy()\n",
    "    inactive_count = tf.data.experimental.cardinality(inactive_dataset).numpy()\n",
    "    \n",
    "    print(f\"\\n--- Statistics (Original) ---\")\n",
    "    print(f\"Active Samples: {active_count}, Inactive Samples: {inactive_count}\")\n",
    "\n",
    "    if active_count == 0 or inactive_count == 0:\n",
    "        print(\"Warning: One class has zero samples. Cannot create a balanced sample.\")\n",
    "        return tf.data.Dataset.from_tensor_slices((tf.constant([]), tf.constant([])))\n",
    "\n",
    "    # Determine the minority class count\n",
    "    min_class_count = min(active_count, inactive_count)\n",
    "    \n",
    "    # Calculate the desired number of samples for each class in the balanced dataset\n",
    "    # This aims for (fraction_to_sample * total_original_count) / 2 samples per class\n",
    "    desired_samples_per_class = int(min_class_count * fraction_to_sample)\n",
    "    \n",
    "    # Sample from each class\n",
    "    # .take() is used for deterministic sampling. For random sampling, you'd shuffle first.\n",
    "    active_sampled = active_dataset.shuffle(buffer_size=1024).take(desired_samples_per_class)\n",
    "    inactive_sampled = inactive_dataset.shuffle(buffer_size=1024).take(desired_samples_per_class)\n",
    "    \n",
    "    # Union the sampled datasets\n",
    "    balanced_dataset = active_sampled.union(inactive_sampled).shuffle(buffer_size=1024)\n",
    "    \n",
    "    return balanced_dataset\n",
    "\n",
    "# Apply balanced sampling (conceptual)\n",
    "df_train_balanced_tf = balanced_sample_tf_dataset(train_dataset, fraction_to_sample=0.1)\n",
    "df_val_balanced_tf = balanced_sample_tf_dataset(val_dataset, fraction_to_sample=0.1)\n",
    "\n",
    "# Verify final counts (requires iterating the dataset, can be slow)\n",
    "final_train_active_count = df_train_balanced_tf.filter(lambda s, t: tf.reduce_max(t) >= 1).reduce(0, lambda x, _: x + 1).numpy()\n",
    "final_train_inactive_count = df_train_balanced_tf.filter(lambda s, t: tf.reduce_max(t) < 1).reduce(0, lambda x, _: x + 1).numpy()\n",
    "final_val_active_count = df_val_balanced_tf.filter(lambda s, t: tf.reduce_max(t) >= 1).reduce(0, lambda x, _: x + 1).numpy()\n",
    "final_val_inactive_count = df_val_balanced_tf.filter(lambda s, t: tf.reduce_max(t) < 1).reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "print(\"\\n--- Final Statistics (after conceptual balanced sampling) ---\")\n",
    "print(f\"Training Set Final Active: {final_train_active_count}, Inactive: {final_train_inactive_count}\")\n",
    "print(f\"Validation Set Final Active: {final_val_active_count}, Inactive: {final_val_inactive_count}\")\n",
    "\n",
    "\n",
    "# --- 5. Saving/Loading NumPy Partitions from TensorFlow Dataset (Optional) ---\n",
    "# This mimics the original code's intent to save as NumPy partitions.\n",
    "# In tf.data, you often don't save to intermediate NumPy files if you're\n",
    "# directly feeding into a Keras model. This is for explicit file management.\n",
    "\n",
    "print(\"\\nSaving NumPy partitions from TensorFlow Dataset (conceptual)...\")\n",
    "# This part would involve iterating through the TF dataset and saving each sample/target\n",
    "# as a separate .npy file. For very large datasets, this can be slow if not parallelized.\n",
    "# You would typically use tf.data.experimental.save() for large datasets to a TFRecord format.\n",
    "\n",
    "# Example of saving a few samples (conceptual, not for massive datasets directly)\n",
    "# For a full dataset, you'd use dataset.enumerate() and save each element.\n",
    "# Or, more efficiently, save as TFRecords using tf.data.experimental.save()\n",
    "\n",
    "# This part is left commented out as it's a manual process for large datasets.\n",
    "# For example:\n",
    "# for i, (sample, target) in enumerate(df_train_balanced_tf.take(5)): # Take first 5 for example\n",
    "#     np.save(os.path.join(train_sample_dir, f\"sample_{i}.npy\"), sample.numpy())\n",
    "#     np.save(os.path.join(train_target_dir, f\"target_{i}.npy\"), target.numpy())\n",
    "# print(\"Sample NumPy partitions saved.\")\n",
    "\n",
    "# --- 6. Model Definition (using Keras/TensorFlow) ---\n",
    "# This part remains largely the same as your original TensorFlow model definition.\n",
    "# The 'FeedBack' and 'FeedBackAttetion' classes would be defined elsewhere in your code.\n",
    "print(\"\\nDefining LSTM model...\")\n",
    "# Dummy classes for demonstration if not provided\n",
    "class FeedBack(tf.keras.Model):\n",
    "    def __init__(self, units, out_steps, timesteps, num_features, num_output_features, num_mixtures, stochastic, feature_extraction, kernel):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.out_steps = out_steps\n",
    "        self.timesteps = timesteps\n",
    "        self.num_features = num_features\n",
    "        self.num_output_features = num_output_features\n",
    "        self.num_mixtures = num_mixtures\n",
    "        self.stochastic = stochastic\n",
    "        self.feature_extraction = feature_extraction\n",
    "        self.kernel = kernel\n",
    "        self.dense = tf.keras.layers.Dense(units) # Example layer\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Dummy call for build method\n",
    "        return self.dense(inputs)\n",
    "\n",
    "class FeedBackAttetion(tf.keras.Model):\n",
    "    def __init__(self, units, out_steps, timesteps, num_features, num_output_features, num_mixtures, stochastic, feature_extraction, attention_heads):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.out_steps = out_steps\n",
    "        self.timesteps = timesteps\n",
    "        self.num_features = num_features\n",
    "        self.num_output_features = num_output_features\n",
    "        self.num_mixtures = num_mixtures\n",
    "        self.stochastic = stochastic\n",
    "        self.feature_extraction = feature_extraction\n",
    "        self.attention_heads = attention_heads\n",
    "        self.dense = tf.keras.layers.Dense(units) # Example layer\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Dummy call for build method\n",
    "        return self.dense(inputs)\n",
    "\n",
    "def get_lstm_model(timesteps, num_features, num_output_features, num_mixtures, units, conv_kernel):\n",
    "    # This is a placeholder for your actual model definition function\n",
    "    model = FeedBack(units=units, out_steps=2, timesteps=timesteps, num_features=num_features,\n",
    "                     num_output_features=num_output_features, num_mixtures=num_mixtures,\n",
    "                     stochastic=True, feature_extraction=True, kernel=conv_kernel)\n",
    "    return model\n",
    "\n",
    "# Your model instantiation\n",
    "lstm_model = FeedBack(units=16,\n",
    "                      out_steps=2,\n",
    "                      timesteps=4,\n",
    "                      num_features=22,\n",
    "                      num_output_features=1,\n",
    "                      num_mixtures=128,\n",
    "                      stochastic=True,\n",
    "                      feature_extraction=True,\n",
    "                      kernel=3\n",
    "                      )\n",
    "lstm_model.build(input_shape=(None, lstm_model.timesteps, lstm_model.num_features))\n",
    "print(\"UserWarning: `build()` called on layer 'feed_back'...\") # Acknowledge the warning\n",
    "\n",
    "lstm_model_attention = FeedBackAttetion(\n",
    "    units=16,\n",
    "    out_steps=2,\n",
    "    timesteps=4,\n",
    "    num_features=22,\n",
    "    num_output_features=1,\n",
    "    num_mixtures=128,\n",
    "    stochastic=True,\n",
    "    feature_extraction=True,\n",
    "    attention_heads=3,)\n",
    "lstm_model_attention.build(input_shape=(None, 4, 22))\n",
    "print(\"UserWarning: `build()` called on layer 'feed_back_attention'...\") # Acknowledge the warning\n",
    "\n",
    "lstm_model_get = get_lstm_model(timesteps=4, num_features=22, num_output_features=1, num_mixtures=128, units=16, conv_kernel=3)\n",
    "lstm_model_get.summary()\n",
    "\n",
    "print(\"\\nTensorFlow-only data pipeline and model setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ea443-8ded-43c7-9970-850139b1acc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Define paths for NumPy partitions ---\n",
    "train_numpy_path = os.path.join(base_path, f\"Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/train\")\n",
    "val_numpy_path = os.path.join(base_path, f\"Dati/USE_CASE/pChurn/numpy_partitions/{df_prefix}/val\")\n",
    "\n",
    "train_sample_path = os.path.join(train_numpy_path, \"sample\")\n",
    "train_target_path = os.path.join(train_numpy_path, \"target\")\n",
    "val_sample_path = os.path.join(val_numpy_path, \"sample\")\n",
    "val_target_path = os.path.join(val_numpy_path, \"target\")\n",
    "\n",
    "# --- Clean up existing directories ---\n",
    "if os.path.exists(train_numpy_path):\n",
    "    shutil.rmtree(train_numpy_path)\n",
    "if os.path.exists(val_numpy_path):\n",
    "    shutil.rmtree(val_numpy_path)\n",
    "\n",
    "# --- Create directories ---\n",
    "os.makedirs(train_sample_path, exist_ok=True)\n",
    "os.makedirs(train_target_path, exist_ok=True)\n",
    "os.makedirs(val_sample_path, exist_ok=True)\n",
    "os.makedirs(val_target_path, exist_ok=True)\n",
    "\n",
    "# --- Data Loading and Processing (Conceptual - this is the part that changes most) ---\n",
    "# This is the *hardest* part to replace directly with \"only TensorFlow\"\n",
    "# You would typically:\n",
    "# 1. Have your data already in a TF-friendly format (e.g., TFRecord files, CSVs, or individual NumPy files).\n",
    "# 2. Use tf.data.Dataset.from_tensor_slices or tf.data.TFRecordDataset etc.\n",
    "# 3. Perform any \"column\" additions or transformations using tf.map or Python.\n",
    "\n",
    "# Example if data is already in NumPy arrays (simplified, not reading parquet here)\n",
    "# For example, if you had train_features.npy, train_labels.npy, etc.\n",
    "# train_samples = np.load(os.path.join(base_path, f\"Dati/USE_CASE/pChurn/{df_prefix}_df_train_samples.npy\"))\n",
    "# train_targets = np.load(os.path.join(base_path, f\"Dati/USE_CASE/pChurn/{df_prefix}_df_train_targets.npy\"))\n",
    "# val_samples = np.load(os.path.join(base_path, f\"Dati/USE_CASE/pChurn/{df_prefix}_df_val_samples.npy\"))\n",
    "# val_targets = np.load(os.path.join(base_path, f\"Dati/USE_CASE/pChurn/{df_prefix}_df_val_targets.npy\"))\n",
    "\n",
    "# Then, you'd save these into the new partitioned structure\n",
    "# np.save(os.path.join(train_sample_path, \"data.npy\"), train_samples)\n",
    "# np.save(os.path.join(train_target_path, \"data.npy\"), train_targets)\n",
    "# np.save(os.path.join(val_sample_path, \"data.npy\"), val_samples)\n",
    "# np.save(os.path.join(val_target_path, \"data.npy\"), val_targets)\n",
    "\n",
    "# If your original parquet files are small enough to fit in memory,\n",
    "# you could use pandas to read them, then convert to numpy/tf.data\n",
    "# import pandas as pd\n",
    "# df_train_pd = pd.read_parquet(base_path + f\"Dati/USE_CASE/pChurn/{df_prefix}_df_train.parquet/\")\n",
    "# df_val_pd = pd.read_parquet(base_path + f\"Dati/USE_CASE/pChurn/{df_prefix}_df_val.parquet/\")\n",
    "\n",
    "# Then convert pandas DataFrames to TensorFlow Datasets or NumPy arrays\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices(dict(df_train_pd))\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices(dict(df_val_pd))\n",
    "\n",
    "# For adding \"path\", \"row_idx\", \"partition_idx\" - these are Spark DataFrame concepts.\n",
    "# In TensorFlow, you'd manage file paths explicitly when saving/loading,\n",
    "# and row/partition indices would be implicit in your tf.data pipeline or file organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41749860-f91a-4210-9387-2638edbaf141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spark_partition_to_tensors(partition_iterator):\n",
    "    # Converte l'iteratore della partizione in un DataFrame pandas (efficiente a livello di partizione)\n",
    "    pdf = pd.DataFrame([row.asDict() for row in partition_iterator])\n",
    "    \n",
    "    # Se la partizione è vuota, esci.\n",
    "    if pdf.empty:\n",
    "        return\n",
    "\n",
    "    # Estrai le colonne 'sample' e 'target' e convertile in array NumPy pronti per TF\n",
    "    # np.stack converte una lista di array in un unico array multidimensionale.\n",
    "    samples = np.stack(pdf[\"sample\"].apply(lambda x: x.toArray()).values)\n",
    "    targets = np.stack(pdf[\"target\"].values)\n",
    "    \n",
    "    # \"Yield\" crea un generatore, che è efficiente in termini di memoria.\n",
    "    # Restituisce una singola tupla (samples, targets) per l'intera partizione.\n",
    "    yield samples, targets\n",
    "\n",
    "# --- 2. CREA I TF.DATA.DATASET USANDO UN GENERATORE (Metodo Corretto) ---\n",
    "\n",
    "# Ottieni le specifiche dei dati per aiutare TensorFlow a capire la struttura\n",
    "# Il primo 'None' rappresenta la dimensione del batch, che può variare.\n",
    "sample_spec = tf.TensorSpec(shape=(None, 4, 22), dtype=tf.float32)\n",
    "target_spec = tf.TensorSpec(shape=(None, 2), dtype=tf.float32)\n",
    "output_signature = (sample_spec, target_spec)\n",
    "\n",
    "# Crea un \"generatore\" che tira i dati dai worker una partizione alla volta\n",
    "# .toLocalIterator() è memory-efficient: non carica tutto il dataset sul driver.\n",
    "train_generator = lambda: df_train.rdd.mapPartitions(spark_partition_to_tensors).toLocalIterator()\n",
    "\n",
    "# Crea la pipeline di dati di TensorFlow usando il generatore\n",
    "# TensorFlow ora chiederà a Spark le partizioni una per una, man mano che servono.\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    train_generator,\n",
    "    output_signature=output_signature\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Fai lo stesso per il validation set\n",
    "val_generator = lambda: df_val.rdd.mapPartitions(spark_partition_to_tensors).toLocalIterator()\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    val_generator,\n",
    "    output_signature=output_signature\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Pipeline di dati TensorFlow create correttamente in memoria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554edc2-7559-44d4-a71f-f7f582fcf53e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(train_x.shape, train_y.shape, df_train.count())\n",
    "#print((val_tuple[0]).shape, (val_tuple[1]).shape, df_val.count())\n",
    "\n",
    "# Otteniamo il conteggio totale dei campioni direttamente dai DataFrame Spark.\n",
    "# Questa è un'operazione veloce che non causa problemi di memoria.\n",
    "train_count = df_train.count()\n",
    "val_count = df_val.count()\n",
    "\n",
    "print(f\"Numero totale di campioni di training: {train_count}\")\n",
    "print(f\"Numero totale di campioni di validazione: {val_count}\")\n",
    "\n",
    "# La forma di un singolo campione (es. (4, 22)) è definita dalla logica di preprocessing\n",
    "print(f\"Forma di un singolo campione (sample): (4, 22)\")\n",
    "print(f\"Forma di un singolo target: (2, 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd7f45-af18-4641-89a7-3167c3e66ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
